# Correlation, Regression & PCA in Carm: Technical Report

## A Complete Implementation of Pearson/Spearman/Kendall Correlation, OLS/Logistic Regression, Diagnostics, and PCA with Varimax Rotation in TypeScript

**Date:** 2026-02-26
**Version:** Carm 1.0
**Module:** `src/stats/correlation.ts` (283 lines), `src/stats/regression.ts` (379 lines), `src/stats/pca.ts` (157 lines)
**Total:** ~819 LOC across 3 files
**Dependencies:** Zero external math libraries — all linear algebra, distributions, and statistical computations from `core/math.ts` and `core/matrix.ts`
**Validation:** Cross-validated with R's `cor.test()`, `lm()`, `glm()`, `prcomp()`, `varimax()`

---

## Table of Contents

1. [Architecture & Design Principles](#1-architecture--design-principles)
2. [Pearson Correlation](#2-pearson-correlation)
3. [Spearman Rank Correlation](#3-spearman-rank-correlation)
4. [Kendall's Tau-b](#4-kendalls-tau-b)
5. [Partial Correlation](#5-partial-correlation)
6. [Correlation Matrix](#6-correlation-matrix)
7. [OLS Engine](#7-ols-engine)
8. [Simple Linear Regression](#8-simple-linear-regression)
9. [Multiple Linear Regression](#9-multiple-linear-regression)
10. [Polynomial Regression](#10-polynomial-regression)
11. [Logistic Regression via IRLS](#11-logistic-regression-via-irls)
12. [Regression Diagnostics](#12-regression-diagnostics)
13. [PCA via SVD](#13-pca-via-svd)
14. [Varimax Rotation](#14-varimax-rotation)
15. [Public API Reference](#15-public-api-reference)
16. [References](#16-references)
17. [Engineering Decisions: Problems, Solutions, and Optimizations](#17-engineering-decisions-problems-solutions-and-optimizations)
18. [Mathematical Tricks That Made It Possible](#18-mathematical-tricks-that-made-it-possible)

---

## 1. Architecture & Design Principles

### 1.1 Linear Algebra Pipeline

The three modules form a natural dependency chain built on a shared linear algebra foundation:

```
core/math.ts  ──→  correlation.ts  ──→  regression.ts  ──→  pca.ts
core/matrix.ts ─┘    (r, p, CI)       (OLS, logistic)    (SVD, varimax)
```

All three modules consume the `Matrix` class from `core/matrix.ts` for linear algebra operations (transpose, multiply, inverse, SVD) and distribution functions from `core/math.ts` (t-distribution p-values, normal CDF, quantiles). No external numerical libraries are used anywhere.

### 1.2 Shared OLS Engine

The `fitOLS()` function (regression.ts, lines 21–106) serves as the central engine for all linear regression variants. Simple linear, multiple, and polynomial regression differ only in how they construct the design matrix — the estimation, inference, and diagnostics pipeline is identical. This eliminates code duplication and ensures consistent behavior across regression types.

### 1.3 Pure Functions, Structured Results

Every public function is pure: it takes data in and returns a structured result object. No side effects, no DOM manipulation, no state mutation. All correlation functions return `StatResult` objects with `{ testName, statistic, df, pValue, effectSize, ci, ciLevel, n, formatted }`. All regression functions return `RegressionResult` objects with `{ coefficients, r2, adjR2, fStatistic, fDf, fPValue, aic, bic, residuals, fitted, n, formatted }`.

The `formatted` field provides a ready-to-display APA-style string (e.g., `r(48) = .35, p = .025, 95% CI [-.62, .91]`), generated by `core/apa.ts`.

### 1.4 Input Validation

Every public function validates inputs at the boundary with descriptive, function-name-prefixed errors:

- Array length mismatches: `'pearsonCorrelation: arrays must have equal length'`
- Insufficient observations: `'pearsonCorrelation: need at least 3 observations'`
- Zero variance: `'pearsonCorrelation: zero variance in input'`
- Insufficient degrees of freedom: `'fitOLS: not enough degrees of freedom'`
- Invalid outcome values: `'logisticRegression: y must be 0 or 1'`

---

## 2. Pearson Correlation

### 2.1 The Formula

Pearson's product-moment correlation coefficient (correlation.ts, lines 30–67):

```
r = cov(x, y) / (sd(x) * sd(y))
```

Where `cov()` computes the sample covariance `Σ(x_i - x̄)(y_i - ȳ) / (n-1)` and `sd()` computes the sample standard deviation. Both are imported from `core/math.ts`.

### 2.2 Floating-Point Guard: r Clamping

After computing r from the ratio of covariance to the product of standard deviations, the result is clamped to [-1, 1]:

```typescript
const rClamped = Math.max(-1, Math.min(1, r))
```

This prevents floating-point arithmetic from producing |r| > 1 (e.g., r = 1.0000000000000002), which would cause `sqrt(1 - r*r)` to return NaN and propagate through all downstream computations.

### 2.3 t-Statistic and Perfect Correlation Guard

The test statistic for H_0: ρ = 0 is:

```
t = r * sqrt((n-2) / (1 - r²))
```

When |r| = 1 exactly, the denominator `1 - r²` becomes 0, producing division by zero. The implementation guards against this:

```typescript
const t = Math.abs(rClamped) === 1 ? Infinity : rClamped * Math.sqrt(df / (1 - rClamped * rClamped))
```

Setting t = Infinity ensures `tDistPValue(Infinity, df)` returns p = 0, which is the correct result: a perfect correlation rejects the null hypothesis at any significance level.

### 2.4 Fisher z-Transform Confidence Interval

The confidence interval for r uses Fisher's z-transform (correlation.ts, lines 70–81):

```
z = 0.5 * ln((1 + r) / (1 - r))     // Fisher's z — maps r ∈ (-1,1) to z ∈ (-∞,+∞)
SE = 1 / sqrt(n - 3)                  // Standard error of z
z_crit = Φ⁻¹(1 - α/2)               // Normal quantile
CI_z = [z - z_crit * SE, z + z_crit * SE]
CI_r = [tanh(CI_z_lo), tanh(CI_z_hi)] // Back-transform via tanh
```

The back-transformation via `Math.tanh()` guarantees the CI bounds are always in [-1, 1], unlike a naive normal approximation on r which can exceed these bounds for large |r|.

---

## 3. Spearman Rank Correlation

### 3.1 Pearson-on-Ranks Implementation

Spearman's rank correlation (correlation.ts, lines 92–114) is implemented as Pearson's r computed on rank-transformed data:

```typescript
const rx = rank(x), ry = rank(y)
const rhoResult = pearsonCorrelation(rx, ry, ciLevel)
```

### 3.2 Why Pearson-on-Ranks, Not the D² Formula

The classic textbook formula `ρ = 1 - 6Σd²/(n(n²-1))` (where d_i = rank(x_i) - rank(y_i)) only works when there are no ties. When ties exist, the formula requires correction terms that are complex and error-prone.

The Pearson-on-ranks approach handles ties naturally because `rank()` from `core/math.ts` (math.ts, lines 514–528) assigns average ranks to tied values:

```typescript
// rank() assigns average ranks for ties:
// [10, 20, 20, 30] → [1, 2.5, 2.5, 4]
const avgRank = (i + j - 1) / 2 + 1
```

Pearson's r on these averaged ranks produces the correct tie-corrected Spearman coefficient, with no additional formula needed.

### 3.3 Result Structure

The result inherits all fields from the Pearson computation (t-statistic, p-value, Fisher z CI) but overrides `testName` to `"Spearman's ρ"` and `effectSize.name` to `"Spearman's ρ"`.

---

## 4. Kendall's Tau-b

### 4.1 Pairwise Concordance Counting

Kendall's tau-b (correlation.ts, lines 125–176) uses an O(n²) pairwise comparison approach:

```typescript
for (let i = 0; i < n; i++) {
  for (let j = i + 1; j < n; j++) {
    const dx = x[i] - x[j]
    const dy = y[i] - y[j]
    const sign = Math.sign(dx * dy)
    if (sign > 0) concordant++
    else if (sign < 0) discordant++
    if (dx === 0) tiesX++
    if (dy === 0) tiesY++
  }
}
```

For each pair (i, j), the pair is concordant if x and y move in the same direction, discordant if they move in opposite directions, and a tie on x or y if the respective values are equal.

### 4.2 Tau-b Formula with Tie Correction

The tau-b statistic corrects for ties in both variables:

```
τ_b = (C - D) / sqrt((n₂ - T_x)(n₂ - T_y))
```

Where C = concordant pairs, D = discordant pairs, n₂ = n(n-1)/2 (total pairs), T_x = ties on x, T_y = ties on y. Without the tie correction in the denominator, the statistic would be deflated toward zero when ties are present.

### 4.3 Normal Approximation p-Value

The p-value uses a normal approximation with the variance formula:

```
Var(τ) = 2(2n + 5) / (9n(n - 1))
z = τ / sqrt(Var(τ))
p = 2(1 - Φ(|z|))
```

This is the standard large-sample approximation, adequate for n >= 10.

### 4.4 Why O(n²) and Not O(n log n)

An O(n log n) algorithm exists based on merge-sort counting of inversions (Knight, 1966). The O(n²) approach was chosen because:

1. It is dramatically simpler to implement and verify
2. For typical social science sample sizes (n < 10,000), the difference is negligible
3. The tie-handling in the merge-sort variant is complex and easy to get wrong
4. Cross-validation against R is easier with a straightforward algorithm

---

## 5. Partial Correlation

### 5.1 OLS Residualization Approach

Partial correlation (correlation.ts, lines 187–198) controls for confounding variables by residualizing both x and y on the control variables, then correlating the residuals:

```typescript
export function partialCorrelation(
  x: readonly number[],
  y: readonly number[],
  controls: readonly (readonly number[])[]
): StatResult {
  const xRes = residualize(x, controls)
  const yRes = residualize(y, controls)
  return pearsonCorrelation(xRes, yRes)
}
```

### 5.2 The Residualize Function

The `residualize()` helper (correlation.ts, lines 201–214) fits an OLS regression of the target variable on the control variables and returns the residuals:

```typescript
function residualize(y: readonly number[], predictors: readonly (readonly number[])[]): number[] {
  if (predictors.length === 0) return [...y]
  const X = Matrix.fromArray(/* [1, z1, z2, ...] */)
  const beta = XtX.inverse().multiply(XtY)
  const fitted = X.multiply(beta)
  return Array.from({ length: n }, (_, i) => y[i] - fitted.get(i, 0))
}
```

### 5.3 Why Residualization Over the Recursive Formula

The textbook first-order partial correlation formula is:

```
r_xy.z = (r_xy - r_xz * r_yz) / sqrt((1 - r_xz²)(1 - r_yz²))
```

This only handles a single control variable. For multiple controls, you must apply the formula recursively — partial out one variable at a time, recompute correlations on the partially controlled data, then partial out the next variable. This is error-prone and numerically unstable for many controls.

The OLS residualization approach generalizes to any number of control variables in a single step: fit one regression per variable (x on all controls, y on all controls), take residuals, correlate. The result is mathematically equivalent to the recursive formula but simpler, more stable, and naturally handles multicollinearity among controls.

### 5.4 Degrees of Freedom

The p-value from `pearsonCorrelation(xRes, yRes)` uses df = n - 2 (from the Pearson test on residuals). The correct degrees of freedom for partial correlation should be df = n - 2 - k where k is the number of control variables. This is a known limitation — the p-value is slightly anti-conservative when many controls are used.

---

## 6. Correlation Matrix

### 6.1 Pairwise Construction

The correlation matrix function (correlation.ts, lines 229–282) computes all pairwise correlations between k variables:

```typescript
export function correlationMatrix(
  data: readonly (readonly number[])[],
  labels?: readonly string[],
  method: 'pearson' | 'spearman' | 'kendall' = 'pearson'
): CorrelationMatrix
```

The method parameter selects the correlation function (`pearsonCorrelation`, `spearmanCorrelation`, or `kendallTau`), and the same function is applied to every pair.

### 6.2 Error Handling per Pair

Each pairwise computation is wrapped in a try-catch:

```typescript
try {
  return corrFn(data[i]!, data[j]!).statistic
} catch {
  return NaN
}
```

If a pair fails (e.g., zero variance in one variable, insufficient non-missing observations), the correlation is set to NaN and the p-value to NaN rather than throwing and aborting the entire matrix computation. This graceful degradation ensures that one problematic variable does not prevent analysis of all other variables.

### 6.3 Symmetric Fill

The upper triangle is computed first, then the lower triangle is filled by symmetry:

```typescript
for (let i = 0; i < k; i++) {
  for (let j = 0; j < i; j++) {
    r[i][j] = r[j][i]
    pValues[i][j] = pValues[j][i]
  }
}
```

Diagonal elements are r = 1 and p = NaN (no test for self-correlation).

---

## 7. OLS Engine

### 7.1 Normal Equations

The core OLS engine (regression.ts, lines 21–106) solves the normal equations:

```
β = (X'X)⁻¹ X'y
```

Where X is the n x p design matrix (with intercept column) and y is the response vector.

```typescript
const Xt = X.transpose()
const XtX = Xt.multiply(X)
const XtY = Xt.multiply(Matrix.colVec(y))
const XtXInv = XtX.inverse()
const betaM = XtXInv.multiply(XtY)
```

### 7.2 R-Squared and Adjusted R-Squared

R-squared is computed from residual and total sums of squares with a non-negativity clamp:

```typescript
const r2 = ss_tot > 0 ? Math.max(0, 1 - ss_res / ss_tot) : 0
const adjR2 = 1 - (1 - r2) * (n - 1) / (n - p)
```

The `Math.max(0, ...)` clamp prevents floating-point errors from producing negative R² values, which can occur when the model is worse than the mean (should only happen with no-intercept models, but floating-point arithmetic can produce tiny negative values even with an intercept).

### 7.3 Coefficient Standard Errors

Standard errors come from the diagonal of the variance-covariance matrix of the coefficients:

```typescript
const sigma2 = ss_res / dfRes           // MSE = RSS / (n - p)
const covBeta = XtXInv.scale(sigma2)    // Var(β) = σ² * (X'X)⁻¹
const se = Math.sqrt(Math.max(0, covBeta.get(i, i)))
```

The `Math.max(0, ...)` guard prevents `sqrt` of a negative number, which can occur when the diagonal element of (X'X)^-1 is slightly negative due to floating-point error in the matrix inversion.

### 7.4 F-Statistic

The overall model F-statistic tests H_0: all slopes = 0:

```
F = (SS_reg / df_model) / (SS_res / df_res)
```

Where df_model = p - 1 (excluding intercept) and df_res = n - p. The p-value comes from the F-distribution via `fDistPValue()`.

### 7.5 Information Criteria

AIC and BIC are computed from the concentrated log-likelihood for normal errors:

```typescript
const rssSafe = Math.max(ss_res, 1e-15)
const logLik = -n / 2 * (Math.log(2 * Math.PI) + Math.log(rssSafe / n) + 1)
const aic = -2 * logLik + 2 * (p + 1)
const bic = -2 * logLik + Math.log(n) * (p + 1)
```

The `Math.max(ss_res, 1e-15)` prevents `log(0) = -Infinity` on a perfect fit (where RSS = 0 exactly). The penalty counts p + 1 parameters (p coefficients plus the error variance σ²).

---

## 8. Simple Linear Regression

Simple linear regression (regression.ts, lines 113–123) constructs a 2-column design matrix [1, x] and routes through `fitOLS()`:

```typescript
export function linearRegression(x, y, ciLevel = 0.95): RegressionResult {
  const X = Matrix.fromArray(Array.from({ length: n }, (_, i) => [1, x[i]]))
  return fitOLS(X, y, ['(Intercept)', 'x'], ciLevel)
}
```

The result contains slope (`coefficients[1]`), intercept (`coefficients[0]`), and all standard OLS diagnostics (R², F, AIC, BIC, residuals, fitted values).

---

## 9. Multiple Linear Regression

Multiple regression (regression.ts, lines 131–147) accepts named predictor columns and constructs the design matrix [1, x1, x2, ...]:

```typescript
export function multipleRegression(y, predictors, ciLevel = 0.95): RegressionResult {
  const X = Matrix.fromArray(
    Array.from({ length: n }, (_, i) => [1, ...predictors.map(p => p.values[i])])
  )
  const names = ['(Intercept)', ...predictors.map(p => p.name)]
  return fitOLS(X, y, names, ciLevel)
}
```

Each predictor is validated for length consistency against y before matrix construction.

---

## 10. Polynomial Regression

Polynomial regression (regression.ts, lines 154–170) constructs the design matrix [1, x, x², ..., x^degree]:

```typescript
export function polynomialRegression(x, y, degree, ciLevel = 0.95): RegressionResult {
  const X = Matrix.fromArray(
    Array.from({ length: n }, (_, i) =>
      [1, ...Array.from({ length: degree }, (_, d) => x[i] ** (d + 1))]
    )
  )
  const names = ['(Intercept)', ...Array.from({ length: degree }, (_, d) => `x^${d + 1}`)]
  return fitOLS(X, y, names, ciLevel)
}
```

Since polynomial regression is just OLS with derived predictors, all the same diagnostics apply: R², AIC, BIC, coefficient CIs, residuals.

---

## 11. Logistic Regression via IRLS

### 11.1 The IRLS Algorithm

Binary logistic regression (regression.ts, lines 181–304) uses Fisher scoring, which is equivalent to Iteratively Reweighted Least Squares (IRLS). Starting from β = 0, each iteration solves a weighted least squares problem:

```
(X'WX) Δβ = X'W(y - μ)
```

Where:
- η = Xβ (linear predictor)
- μ = logistic(η) = 1 / (1 + exp(-η)) (predicted probabilities)
- W = diag(μ(1-μ)) (weight matrix)

### 11.2 Implementation via Square Root Weights

Rather than forming W explicitly (an n x n diagonal matrix), the implementation uses square-root weights to convert to a standard OLS form:

```typescript
const Xw = Matrix.fromArray(
  Array.from({ length: n }, (_, i) =>
    Array.from({ length: p }, (_, j) => X.get(i, j) * Math.sqrt(w[i]!)))
)
const yAdj = Array.from({ length: n }, (_, i) =>
  Math.sqrt(w[i]!) * ((y[i]) - (mu[i])))
```

This computes X_w = W^(1/2) X and y_adj = W^(1/2)(y - μ), then solves (X_w'X_w)Δβ = X_w'y_adj, which is mathematically equivalent to the weighted normal equations but avoids forming the n x n diagonal W.

### 11.3 Weight Clamping

The weights w_i = μ_i(1 - μ_i) are clamped to a minimum of 1e-10:

```typescript
const w = mu.map(m => Math.max(1e-10, m * (1 - m)))
```

Without this clamp, when μ approaches 0 or 1 (which happens when separation is near-complete), w approaches 0, making X'WX singular. The clamped weight ensures X'WX remains invertible throughout the iteration.

### 11.4 Convergence

Convergence is checked on the maximum absolute change in any coefficient:

```typescript
let maxChange = 0
for (let j = 0; j < p; j++) {
  const d = delta.get(j, 0)
  beta[j] += d
  maxChange = Math.max(maxChange, Math.abs(d))
}
if (maxChange < tol) break  // tol = 1e-8
```

Maximum iterations default to 100. If the matrix becomes singular during iteration (caught by the `try/catch` around the inverse), iteration stops early with the current coefficients.

### 11.5 Coefficient Inference: Wald z-Test

After convergence, standard errors are computed from the Fisher information matrix (X'WX)^-1:

```typescript
const se = Math.sqrt(Math.max(0, cov.get(i, i)))
const z = se === 0 ? 0 : b / se
const pVal = 2 * (1 - normCDFLocal(Math.abs(z)))
```

The Wald z-test is used (normal approximation) rather than the likelihood ratio test. This matches R's `glm()` default output for `summary()`.

### 11.6 McFadden Pseudo-R²

The McFadden pseudo-R² measures model fit relative to the null (intercept-only) model:

```typescript
const r2 = Math.abs(nullLogLik) < 1e-12 ? NaN : 1 - logLik / nullLogLik
```

The NaN guard handles the degenerate case where all observations belong to the same class (e.g., all y = 1). In this case the null log-likelihood approaches 0, making the ratio undefined. Returning NaN rather than a meaningless number signals that the pseudo-R² is not interpretable for degenerate outcomes.

### 11.7 Null Log-Likelihood Clamping

The null model's predicted probability is clamped away from 0 and 1:

```typescript
const pMean = Math.min(1 - 1e-12, Math.max(1e-12, pMeanRaw))
```

This prevents `log(0) = -Infinity` when all observations are in the same class (pMeanRaw = 0 or 1), which would make the null log-likelihood undefined and break the AIC/BIC computation.

---

## 12. Regression Diagnostics

### 12.1 Hat Matrix Diagonal (Leverage)

Leverage values (regression.ts, lines 327–345) come from the diagonal of the hat matrix H = X(X'X)^-1 X':

```typescript
const hat = X.multiply(XtXInv).multiply(Xt)
const leverage = Array.from({ length: n }, (_, i) => hat.get(i, i))
```

h_ii measures how far observation i's predictor values are from the centroid of the predictor space. High-leverage points (h_ii >> 2p/n) have outsized influence on the regression.

### 12.2 Standardized Residuals

Standardized residuals divide raw residuals by their estimated standard deviation:

```typescript
const denom = Math.sqrt(mse * (1 - leverage[i]))
return denom === 0 ? 0 : r / denom
```

The factor (1 - h_ii) accounts for the fact that high-leverage points tend to have smaller residuals (the regression line is pulled toward them).

### 12.3 Cook's Distance

Cook's distance combines leverage and residual size to measure overall influence:

```typescript
const cooksDistance = result.residuals.map((r, i) => {
  const h = leverage[i]
  return (r * r * h) / (p * mse * (1 - h) ** 2)
})
```

This is equivalent to the standard formula D_i = (e*_i)^2 * h_ii / (p * (1 - h_ii)), where e*_i is the standardized residual. Points with D_i > 4/n (or D_i > 1 by a stricter criterion) are considered influential.

### 12.4 VIF via Recursive Regression

Variance Inflation Factors (regression.ts, lines 363–375) are computed by regressing each predictor on all other predictors:

```typescript
const vif = predictors.map((_, j) => {
  const otherPreds = predictors.filter((__, k) => k !== j)
  if (otherPreds.length === 0) return 1
  const res = multipleRegression(xj, others)
  return 1 / Math.max(1e-10, 1 - res.r2)
})
```

VIF_j = 1 / (1 - R²_j), where R²_j is the R-squared from regressing predictor j on all other predictors. The `Math.max(1e-10, ...)` guard prevents division by zero when R²_j = 1 (perfect multicollinearity).

---

## 13. PCA via SVD

### 13.1 Data Standardization

PCA (pca.ts, lines 21–70) begins by standardizing the data (centering and scaling to unit variance):

```typescript
const pp = preprocessData(data, { method: scale ? 'standardize' : 'center' })
const X = Matrix.fromArray(pp.data as number[][])
```

When `scale = true` (default), this produces z-scores, making PCA operate on the correlation matrix rather than the covariance matrix. This prevents variables with large scales from dominating the components.

### 13.2 The 1/sqrt(n-1) Scaling Trick

Before computing the SVD, the standardized matrix is scaled:

```typescript
const Xs = X.scale(1 / Math.sqrt(n - 1))
```

This ensures that the singular values of Xs relate directly to the eigenvalues of the covariance matrix. Specifically:

```
X_s = X / sqrt(n-1)
X_s'X_s = X'X / (n-1) = Σ̂  (the sample covariance matrix)
```

If X_s = UΣV', then X_s'X_s = VΣ²V', so the squared singular values S² are the eigenvalues of the covariance matrix, and V contains the eigenvectors (principal component loadings).

### 13.3 SVD Decomposition

The SVD is computed by the one-sided Jacobi algorithm implemented in `Matrix.svd()` (core/matrix.ts, line 226):

```typescript
const { U: _U, S, V } = Xs.svd()
```

The eigenvalues and variance explained are derived from the singular values:

```typescript
const eigenvalues = S.slice(0, nc).map(s => s * s)
const totalVar = S.reduce((sum, s) => sum + s * s, 0)
const varianceExplained = eigenvalues.map(e => totalVar > 0 ? e / totalVar : 0)
```

### 13.4 Loadings and Scores

- **Loadings**: The right singular vectors V — columns are principal components, rows are original variables. `loadings[i][j]` is the weight of variable i on component j.
- **Scores**: Computed as X * V (projecting the centered data onto the principal component axes).

```typescript
const scoresM = X.multiply(Vk)  // n x nc matrix of component scores
```

### 13.5 Cumulative Variance

Cumulative variance explained is computed as a running sum:

```typescript
const cumulativeVariance = varianceExplained.reduce<number[]>((acc, v, i) => {
  acc.push((acc[i - 1] ?? 0) + v)
  return acc
}, [])
```

This supports the standard scree plot and the "% variance explained" threshold for component selection.

---

## 14. Varimax Rotation

### 14.1 Kaiser's Algorithm

Varimax rotation (pca.ts, lines 82–137) implements Kaiser's (1958) pairwise rotation algorithm that maximizes the variance of squared loadings within each factor.

The algorithm iterates over all pairs of components (p, q) and computes the optimal rotation angle:

```typescript
const X_ = C - (A ** 2 - B ** 2) / k
const Y_ = D - 2 * A * B / k
const angle = Math.atan2(Y_, X_) / 4
```

Where A, B, C, D are aggregates of the squared loadings for the pair (p, q). The division by 4 comes from the varimax criterion's quartic objective function.

### 14.2 Pairwise Rotation

Each rotation is a 2D Givens rotation applied to columns p and q of the loading matrix:

```typescript
const cos = Math.cos(angle)
const sin = Math.sin(angle)
const newLp = L.map((row) => row[p] * cos + row[q] * sin)
const newLq = L.map((row) => -row[p] * sin + row[q] * cos)
```

The rotation matrix T is accumulated so that the total transformation is tracked:

```typescript
T[r][p] = T[r][p] * cos + T[r][q] * sin
T[r][q] = -T[r][p] * sin + T[r][q] * cos
```

### 14.3 Convergence

Convergence is measured by the total absolute rotation angle across all pairs in one sweep:

```typescript
delta += Math.abs(angle)
if (delta < tol) break  // tol = 1e-6
```

The algorithm terminates when the total rotation per sweep drops below 1e-6 radians, indicating the solution is stable. Maximum iterations = 1000.

### 14.4 Output

The function returns both the rotated loadings and the rotation matrix:

```typescript
return { rotatedLoadings: L, rotationMatrix: T }
```

The rotation matrix T can be used to rotate new scores: `rotatedScores = scores * T`.

---

## 15. Public API Reference

### 15.1 Correlation (correlation.ts)

```typescript
// Pearson product-moment correlation
pearsonCorrelation(
  x: readonly number[],
  y: readonly number[],
  ciLevel?: number         // default 0.95
): StatResult

// Spearman rank correlation (Pearson on ranks)
spearmanCorrelation(
  x: readonly number[],
  y: readonly number[],
  ciLevel?: number         // default 0.95
): StatResult

// Kendall's tau-b with tie correction
kendallTau(
  x: readonly number[],
  y: readonly number[],
  ciLevel?: number         // default 0.95
): StatResult

// Partial correlation (OLS residualization)
partialCorrelation(
  x: readonly number[],
  y: readonly number[],
  controls: readonly (readonly number[])[]
): StatResult

// Pairwise correlation matrix
correlationMatrix(
  data: readonly (readonly number[])[],
  labels?: readonly string[],
  method?: 'pearson' | 'spearman' | 'kendall'  // default 'pearson'
): CorrelationMatrix
// Returns: { r: number[][], pValues: number[][], n: number, labels: string[] }
```

### 15.2 Regression (regression.ts)

```typescript
// Simple linear regression: y = β₀ + β₁·x
linearRegression(
  x: readonly number[],
  y: readonly number[],
  ciLevel?: number         // default 0.95
): RegressionResult

// Multiple linear regression: y = β₀ + β₁·x₁ + ... + βₖ·xₖ
multipleRegression(
  y: readonly number[],
  predictors: ReadonlyArray<{ name: string; values: readonly number[] }>,
  ciLevel?: number         // default 0.95
): RegressionResult

// Polynomial regression: y = β₀ + β₁·x + ... + βₖ·xᵏ
polynomialRegression(
  x: readonly number[],
  y: readonly number[],
  degree: number,
  ciLevel?: number         // default 0.95
): RegressionResult

// Binary logistic regression via IRLS
logisticRegression(
  y: readonly number[],    // must be 0/1
  predictors: ReadonlyArray<{ name: string; values: readonly number[] }>,
  ciLevel?: number,        // default 0.95
  maxIter?: number,        // default 100
  tol?: number             // default 1e-8
): RegressionResult

// Regression diagnostics
regressionDiagnostics(
  result: RegressionResult,
  predictors: ReadonlyArray<{ name: string; values: readonly number[] }>
): RegressionDiagnostics
// Returns: { leverage, cooksDistance, standardizedResiduals, vif }
```

### 15.3 PCA (pca.ts)

```typescript
// PCA via SVD on standardized data
runPCA(
  data: readonly (readonly number[])[],
  nComponents?: number,    // default: min(n-1, k)
  scale?: boolean          // default: true
): PCAResult
// Returns: { loadings, scores, eigenvalues, varianceExplained, cumulativeVariance, nComponents }

// Varimax rotation of loadings
varimaxRotation(
  loadings: readonly (readonly number[])[],
  maxIter?: number,        // default: 1000
  tol?: number             // default: 1e-6
): { rotatedLoadings: number[][]; rotationMatrix: number[][] }

// Extract scree plot data
screeData(pca: PCAResult): ScreeData
// Returns: { components, eigenvalues, varianceExplained, cumulativeVariance }
```

---

## 16. References

1. **Pearson, K.** (1895). Notes on regression and inheritance in the case of two parents. *Proceedings of the Royal Society of London*, 58, 240–242.

2. **Spearman, C.** (1904). The proof and measurement of association between two things. *The American Journal of Psychology*, 15(1), 72–101.

3. **Kendall, M. G.** (1938). A new measure of rank correlation. *Biometrika*, 30(1/2), 81–93.

4. **Fisher, R. A.** (1921). On the "probable error" of a coefficient of correlation deduced from a small sample. *Metron*, 1, 3–32.

5. **Gauss, C. F.** (1809). *Theoria Motus Corporum Coelestium*. — Legendre, A. M. (1805). *Nouvelles Methodes pour la Determination des Orbites des Cometes*. (Foundations of least squares.)

6. **Berkson, J.** (1944). Application of the logistic function to bio-assay. *Journal of the American Statistical Association*, 39(227), 357–365.

7. **Cox, D. R.** (1958). The regression analysis of binary sequences. *Journal of the Royal Statistical Society, Series B*, 20(2), 215–242.

8. **McCullagh, P. & Nelder, J. A.** (1989). *Generalized Linear Models* (2nd ed.). Chapman & Hall.

9. **McFadden, D.** (1974). Conditional logit analysis of qualitative choice behavior. In P. Zarembka (Ed.), *Frontiers in Econometrics* (pp. 105–142). Academic Press.

10. **Cook, R. D.** (1977). Detection of influential observation in linear regression. *Technometrics*, 19(1), 15–18.

11. **Belsley, D. A., Kuh, E., & Welsch, R. E.** (1980). *Regression Diagnostics: Identifying Influential Data and Sources of Collinearity*. John Wiley & Sons.

12. **Hotelling, H.** (1933). Analysis of a complex of statistical variables into principal components. *Journal of Educational Psychology*, 24(6), 417–441.

13. **Pearson, K.** (1901). On lines and planes of closest fit to systems of points in space. *The London, Edinburgh, and Dublin Philosophical Magazine and Journal of Science*, 2(11), 559–572.

14. **Kaiser, H. F.** (1958). The varimax criterion for analytic rotation in factor analysis. *Psychometrika*, 23(3), 187–200.

---

## 17. Engineering Decisions: Problems, Solutions, and Optimizations

This section documents the engineering problems encountered during development, the decisions made, and how each was resolved. These are practical insights from building production-grade correlation, regression, and PCA modules from scratch.

### 17.1 OLS via Normal Equations, Not QR Decomposition

**Problem:** The standard recommendation for solving OLS in production numerical software is QR decomposition (`β = R⁻¹Q'y`), which is more numerically stable than the normal equations `β = (X'X)⁻¹X'y` when X'X is ill-conditioned.

**Root cause:** The normal equations square the condition number: cond(X'X) = cond(X)². For a design matrix with cond(X) = 10³, cond(X'X) = 10⁶, losing 6 digits of precision.

**Solution:** Use the normal equations anyway, since `Matrix.inverse()` already exists and is well-tested (it uses SVD-based pseudoinverse internally). The `Math.max(0, ...)` guard on coefficient SE handles the rare case where the squared condition number causes a tiny negative diagonal element.

**Why this over QR:** The Matrix class already has a robust `inverse()` method (SVD-based with threshold filtering of near-zero singular values). Implementing a separate QR solver would add ~100 lines of code for a benefit that only manifests with severely ill-conditioned predictors (cond > 10⁶). For typical social science regression (p < 50 predictors, moderate correlations), the normal equations are perfectly adequate.

**Result:** Correct OLS results cross-validated against R's `lm()` for all tested cases. The SVD-based inverse naturally handles moderate ill-conditioning.

### 17.2 R² Clamping to Prevent Negative Values

**Problem:** Theoretically, R² = 1 - SS_res/SS_tot is always in [0, 1] for OLS with an intercept. However, floating-point arithmetic can produce SS_res slightly larger than SS_tot, yielding R² < 0.

**Root cause:** When the model explains almost no variance (R² near 0), the subtraction `SS_tot - SS_res` involves two nearly equal large numbers, losing precision. The result can be slightly negative (e.g., -1e-15).

**Solution:**

```typescript
const r2 = ss_tot > 0 ? Math.max(0, 1 - ss_res / ss_tot) : 0
```

**Why this over alternatives:** The alternative — not clamping and returning -1e-15 — would confuse users and could cause downstream issues (e.g., sqrt(R²) in some formulas). Clamping to 0 is semantically correct: the model explains no variance.

**Result:** R² is always in [0, 1] regardless of floating-point edge cases.

### 17.3 AIC/BIC: `max(SS_res/n, 1e-15)` Prevents log(0)

**Problem:** When the model fits the data perfectly (e.g., fitting n points with n parameters), SS_res = 0. The AIC/BIC formulas involve `log(SS_res/n)`, which becomes `log(0) = -Infinity`.

**Root cause:** Perfect fit is degenerate — the model has zero residual variance, making the Gaussian likelihood infinite. This is mathematically correct but numerically useless.

**Solution:**

```typescript
const rssSafe = Math.max(ss_res, 1e-15)
const logLik = -n / 2 * (Math.log(2 * Math.PI) + Math.log(rssSafe / n) + 1)
```

**Why 1e-15:** This is small enough that it has no effect on any non-degenerate model (even a near-perfect fit with SS_res = 1e-10 is unaffected), but prevents the `-Infinity` that would propagate through all downstream comparisons.

**Result:** AIC and BIC are always finite numbers, even for perfect fits.

### 17.4 Coefficient SE: `sqrt(max(0, ...))` Prevents sqrt of Negative

**Problem:** The variance of coefficient j is `σ² * C_{jj}` where C = (X'X)⁻¹. The diagonal element C_{jj} should always be positive, but floating-point matrix inversion can produce tiny negative values.

**Root cause:** When X'X is moderately ill-conditioned, the inverse can have small numerical errors on the diagonal. For example, C_{jj} = -2e-16 instead of +1e-16.

**Solution:**

```typescript
const se = Math.sqrt(Math.max(0, covBeta.get(i, i)))
```

**Why this over alternatives:** Throwing an error would prevent any output. Returning NaN for the SE would propagate to the t-statistic, p-value, and CI, making the entire coefficient row useless. Clamping to 0 produces SE = 0, t = 0, p = 1 — which correctly indicates that the coefficient is not significantly different from zero (when the variance estimate is unreliable, we should not reject H₀).

**Result:** No NaN propagation from ill-conditioned matrices.

### 17.5 Logistic IRLS Weight Clamping: Why 1e-10

**Problem:** The IRLS weights w_i = μ_i(1 - μ_i) approach 0 when predicted probabilities approach 0 or 1 (complete or near-complete separation). Zero weights make X'WX singular, causing the matrix inverse to fail or produce garbage.

**Root cause:** In early iterations, some observations may have extreme predicted probabilities (μ ≈ 0 or μ ≈ 1), especially when the true relationship is strong. The weights for these observations effectively drop to zero, reducing the effective sample size and potentially making the system underdetermined.

**Solution:**

```typescript
const w = mu.map(m => Math.max(1e-10, m * (1 - m)))
```

**Why 1e-10 and not a larger floor:** The floor must be small enough to not distort the weights for well-determined observations. With n = 100 and most w_i ≈ 0.25, a floor of 1e-10 only activates for truly extreme predictions (μ < 1e-10 or μ > 1 - 1e-10). A larger floor (e.g., 0.01) would bias the estimates by artificially up-weighting observations that should contribute almost nothing.

**What happens without it:** If w_i = 0 for even one observation, the row of X_w corresponding to that observation becomes all zeros. If this happens for too many observations, X_w'X_w becomes singular, `XtXInv` throws, and the iteration stops. The catch block (line 231–233) handles this gracefully by stopping iteration, but the resulting coefficients may be from an early, unconverged iteration.

**Result:** Stable convergence even with near-complete separation. R's `glm()` uses a similar approach (the step-halving in IWLS).

### 17.6 McFadden Pseudo-R²: NaN Guard for Degenerate Outcomes

**Problem:** When all observations belong to the same class (all y = 0 or all y = 1), the null model has pMean = 0 or 1, and the null log-likelihood is n * (0 * log(0) + 1 * log(1)) = 0. McFadden's R² = 1 - LL/LL_null involves division by zero.

**Root cause:** With a homogeneous outcome, there is no variation to explain. Any model will predict the same constant, and the pseudo-R² is meaningless.

**Solution:**

```typescript
const r2 = Math.abs(nullLogLik) < 1e-12 ? NaN : 1 - logLik / nullLogLik
```

Additionally, pMean is clamped away from 0/1 to prevent log(0) in the null log-likelihood computation itself:

```typescript
const pMean = Math.min(1 - 1e-12, Math.max(1e-12, pMeanRaw))
```

**Result:** Degenerate cases return NaN for R² instead of Infinity or -Infinity.

### 17.7 VIF via Recursive Regression: Simple but O(p²n)

**Problem:** Computing VIF for each predictor requires regressing that predictor on all others — effectively running p separate OLS regressions.

**Root cause:** VIF_j = 1/(1 - R²_j) is defined in terms of R²_j, which requires a full OLS fit. No shortcut exists without access to the correlation matrix eigenvalues.

**Solution:** The implementation reuses `multipleRegression()`:

```typescript
const vif = predictors.map((_, j) => {
  const otherPreds = predictors.filter((__, k) => k !== j)
  const res = multipleRegression(xj, others)
  return 1 / Math.max(1e-10, 1 - res.r2)
})
```

**Why this over alternatives:** An alternative is to compute VIF from the diagonal of (X'X)⁻¹: VIF_j = C_{jj} * var(x_j) where X is the predictor matrix without intercept. This is O(p³) for the matrix inverse but avoids p separate regressions. The regression approach was chosen because: (a) `multipleRegression()` already exists and is tested, (b) the per-pair error handling is free (each VIF is independently computed), and (c) p < 50 for typical use cases makes the O(p²n) cost negligible.

**Result:** Correct VIF values, with NaN returned when a regression fails (e.g., singular predictor matrix).

### 17.8 Spearman as Pearson-on-Ranks: Tie Handling for Free

**Problem:** The classic Spearman formula `ρ = 1 - 6Σd²/(n(n²-1))` does not handle ties. Ties require correction terms involving the number of tied groups and their sizes.

**Root cause:** The d² formula assumes all ranks are distinct integers 1, 2, ..., n. When ties exist, average ranks are assigned (e.g., 2.5 instead of 2 and 3), and the formula overcorrects.

**Solution:** Implement Spearman as `pearsonCorrelation(rank(x), rank(y))`. The `rank()` function already handles ties by assigning average ranks. Pearson's r on these averaged ranks is the correct tie-corrected Spearman coefficient.

**Why this over the corrected formula:** The corrected formula requires counting the number and size of tied groups, computing correction terms T = (t³ - t)/12 for each group of size t, and adjusting the denominator. This is more code, more potential for bugs, and no more efficient.

**Result:** Correct Spearman coefficients with ties, cross-validated against R's `cor.test(method = "spearman")`.

### 17.9 Fisher z CI: Bounds Guaranteed in [-1, 1]

**Problem:** A naive normal approximation CI for r can exceed [-1, 1]. For example, r = 0.95 with n = 10 gives SE ≈ 0.10, and the upper CI bound would be 0.95 + 1.96 * 0.10 = 1.146 > 1.

**Root cause:** The sampling distribution of r is heavily skewed near the boundaries, making the normal approximation on r invalid for |r| > 0.5.

**Solution:** Fisher's z-transform maps r to z = atanh(r), which is approximately normal with SE = 1/sqrt(n-3). The CI is constructed in z-space and back-transformed via tanh():

```typescript
const z = Math.log((1 + r) / (1 - r)) / 2  // = atanh(r)
const se = 1 / Math.sqrt(n - 3)
return [Math.tanh(lo), Math.tanh(hi)]
```

Since tanh maps (-∞, +∞) to (-1, 1), the CI bounds are always in the valid range.

**Result:** Correct, bounded confidence intervals for r, matching R's `cor.test()`.

### 17.10 Partial Correlation via Residualization: Multiple Controls

**Problem:** The recursive partial correlation formula `r_xy.z = (r_xy - r_xz * r_yz) / sqrt((1-r_xz²)(1-r_yz²))` only handles one control variable at a time. Controlling for multiple variables requires applying the formula recursively — partial out z1, then partial out z2 from the already-partialled correlations, etc.

**Root cause:** The recursive formula operates in correlation space, requiring O(k) rounds of partialling. Each round introduces rounding error, and the order of partialling matters numerically (though not theoretically).

**Solution:** Use OLS residualization: regress x on all controls simultaneously, regress y on all controls simultaneously, correlate the residuals. This handles any number of controls in one step.

**Why this over alternatives:** The OLS approach is: (a) simpler — two regressions plus one correlation, (b) numerically more stable — no recursive accumulation of error, (c) naturally handles collinear controls (the OLS fit absorbs multicollinearity via the (X'X)^-1 term).

**Result:** Correct partial correlations for any number of controls.

### 17.11 PCA X/sqrt(n-1) Scaling: SVD to Covariance Eigenvalues

**Problem:** SVD of the raw centered data matrix X gives singular values that relate to X'X, not to the sample covariance matrix S = X'X/(n-1). The eigenvalues of S (which are the variances of the principal components) are the squared singular values of X divided by (n-1).

**Root cause:** The sample covariance is `S = X'X / (n-1)` (Bessel's correction). SVD of X gives `X = UΣV'`, so `X'X = VΣ²V'`, meaning eigenvalues of X'X are Σ² — off by a factor of (n-1) from eigenvalues of S.

**Solution:** Scale X before SVD:

```typescript
const Xs = X.scale(1 / Math.sqrt(n - 1))
```

Now `Xs'Xs = X'X/(n-1) = S`, so the SVD of Xs gives singular values whose squares are exactly the eigenvalues of the covariance matrix. This matches R's `prcomp(scale. = TRUE)` output.

**Result:** Eigenvalues and variance explained match R's `prcomp()` to machine precision.

### 17.12 SVD vs Eigendecomposition for PCA

**Problem:** PCA can be computed either by eigendecomposing the p x p covariance matrix, or by SVD of the n x p data matrix. Both give the same result (loadings = eigenvectors = right singular vectors).

**Root cause:** The two approaches differ in computational cost and numerical stability:

| Approach | Cost | Numerical properties |
|----------|------|---------------------|
| Eigen of S = X'X/(n-1) | O(p³) eigendecomp + O(n p²) for X'X | Forms X'X explicitly, squaring condition number |
| SVD of X/sqrt(n-1) | O(n p²) SVD | Works directly on X, preserves condition number |

**Solution:** Use SVD. The Jacobi one-sided SVD in `Matrix.svd()` operates on the data matrix without forming the covariance matrix, preserving the original condition number.

**Why this matters:** For wide data (n >> p), the covariance matrix is small and both approaches are fine. But for tall data (n << p) or ill-conditioned data, the SVD approach avoids the numerical amplification of forming X'X.

**Result:** Numerically stable PCA that works correctly even with moderate multicollinearity among variables.

---

## 18. Mathematical Tricks That Made It Possible

### 18.1 IRLS as Iterated Normal Equations

**Why needed:** Logistic regression has no closed-form solution — the log-likelihood is a nonlinear function of β, requiring iterative optimization. Newton-Raphson is the standard approach, but it requires computing the Hessian matrix (second derivatives of the log-likelihood), which is complex to derive and implement.

**The trick:** Fisher scoring replaces the Hessian with its expectation (the Fisher information matrix), which for the Bernoulli likelihood happens to equal X'WX where W = diag(μ(1-μ)). This transforms each Newton step into a weighted least squares problem:

```
β_{t+1} = (X'W_t X)⁻¹ X'W_t z_t
```

where z_t = X β_t + W_t⁻¹(y - μ_t) is the "working response."

Equivalently, each iteration solves for the update Δβ:

```
(X'W_t X) Δβ = X'(y - μ_t)
```

**Implementation:** The square-root form avoids forming W explicitly:

```typescript
const Xw = X * diag(sqrt(w))   // = W^{1/2} X
const yAdj = sqrt(w) * (y - mu) // = W^{1/2}(y - μ)
// Solve: (Xw'Xw) Δβ = Xw'yAdj
```

**Impact:** The entire logistic regression implementation reuses the same OLS normal-equations machinery (Matrix.inverse, Matrix.multiply). No separate optimizer, no gradient computation, no line search. Each IRLS iteration is essentially a call to the OLS solver with modified weights.

### 18.2 Fisher z-Transform: Bounded r to Unbounded z

**Why needed:** The sampling distribution of Pearson's r is skewed and has bounded support [-1, 1]. Standard normal-theory CI construction (estimate +/- z_crit * SE) assumes an approximately normal, unbounded sampling distribution. Applying this directly to r produces CIs that can exceed [-1, 1], especially for large |r| or small n.

**The trick:** Fisher's z-transform maps the bounded correlation to an unbounded, approximately normal quantity:

```
z = 0.5 * ln((1 + r) / (1 - r)) = atanh(r)
```

Key properties:
- z is approximately Normal(atanh(ρ), 1/(n-3)) — the variance depends only on n, not on the true ρ
- The back-transform tanh(z) maps (-∞, +∞) back to (-1, 1), guaranteeing valid CI bounds
- The approximation is excellent even for n as small as 10

**Implementation:**

```typescript
const z = Math.log((1 + r) / (1 - r)) / 2
const se = 1 / Math.sqrt(n - 3)
const zCrit = normalQuantile(1 - (1 - ciLevel) / 2)
return [Math.tanh(z - zCrit * se), Math.tanh(z + zCrit * se)]
```

**Impact:** Correct, bounded CIs for any sample size n >= 4 (n >= 4 ensures n - 3 >= 1, so SE is finite). The implementation is 6 lines of code with no iteration or special cases.

### 18.3 Hat Matrix Diagonal for Cook's Distance

**Why needed:** Cook's distance measures the influence of each observation on the regression — how much all fitted values change when observation i is deleted. The naive computation requires refitting the model n times (once for each leave-one-out deletion), which is O(n * p³).

**The trick:** Cook's D can be expressed entirely in terms of quantities already computed from the full model:

```
D_i = (e*_i)² * h_ii / (p * (1 - h_ii))
```

Where:
- e*_i = e_i / (σ * sqrt(1 - h_ii)) is the standardized residual
- h_ii is the i-th diagonal element of the hat matrix H = X(X'X)⁻¹X'
- p is the number of parameters

The hat matrix diagonal h_ii measures **leverage** — how far observation i's predictor values are from the predictor centroid. The standardized residual measures **outlyingness** in the response. Cook's D combines both: an observation is influential only if it has both high leverage AND a large residual.

**Implementation:**

```typescript
const hat = X.multiply(XtXInv).multiply(Xt)  // H = X(X'X)⁻¹X'
const leverage = Array.from({ length: n }, (_, i) => hat.get(i, i))
const cooksD = residuals.map((r, i) => {
  const h = leverage[i]
  return (r * r * h) / (p * mse * (1 - h) ** 2)
})
```

**Impact:** Cook's D for all n observations from a single matrix computation — no refitting required. Cost is O(n * p²) for the hat matrix, which is dominated by the original OLS fit cost.

### 18.4 Concentrated Log-Likelihood for AIC/BIC

**Why needed:** AIC and BIC require the maximized log-likelihood. For general models, this requires iterative optimization. For the normal linear model, the MLE of σ² has a closed-form expression in terms of RSS, which can be substituted back into the log-likelihood to eliminate σ² — "concentrating" it out.

**The trick:** The normal log-likelihood is:

```
ℓ(β, σ²) = -n/2 * log(2π) - n/2 * log(σ²) - RSS / (2σ²)
```

The MLE σ̂² = RSS/n. Substituting:

```
ℓ(β̂, σ̂²) = -n/2 * (log(2π) + log(RSS/n) + 1)
```

The "+1" comes from RSS / (2 * RSS/n) = n/2. This is a function only of RSS — no iterative maximization needed.

**Implementation:**

```typescript
const logLik = -n / 2 * (Math.log(2 * Math.PI) + Math.log(rssSafe / n) + 1)
const aic = -2 * logLik + 2 * (p + 1)
const bic = -2 * logLik + Math.log(n) * (p + 1)
```

**Impact:** Exact AIC and BIC from a single formula evaluation, no optimization. The penalty term counts p + 1 parameters (p regression coefficients plus the error variance σ²).

---

## Appendix A: Implementation Completeness

| Component | File | Lines | Status |
|-----------|------|-------|--------|
| Pearson correlation | correlation.ts | 30–67 | Cross-validated with R `cor.test()` |
| Fisher z CI | correlation.ts | 70–81 | Bounded, correct for n >= 4 |
| Spearman rank correlation | correlation.ts | 92–114 | Pearson-on-ranks, tie-safe |
| Kendall's tau-b | correlation.ts | 125–176 | O(n²), tie-corrected |
| Partial correlation | correlation.ts | 187–214 | OLS residualization, multi-control |
| Correlation matrix | correlation.ts | 229–282 | Pairwise with error handling |
| OLS engine | regression.ts | 21–106 | Normal equations, full diagnostics |
| Simple linear regression | regression.ts | 113–123 | Routes through OLS |
| Multiple linear regression | regression.ts | 131–147 | Routes through OLS |
| Polynomial regression | regression.ts | 154–170 | Routes through OLS |
| Logistic regression (IRLS) | regression.ts | 181–304 | Fisher scoring, Wald z-test |
| Regression diagnostics | regression.ts | 316–378 | Leverage, Cook's D, VIF |
| PCA via SVD | pca.ts | 21–70 | 1/sqrt(n-1) scaling, matches prcomp() |
| Varimax rotation | pca.ts | 82–137 | Kaiser (1958) pairwise algorithm |
| Scree data extraction | pca.ts | 149–156 | Convenience for visualization |

**Total:** 819 lines of implementation across 3 files.

---

## Appendix B: Known Limitations

1. **OLS via normal equations:** Condition number is squared. For severely ill-conditioned design matrices (condition number > 10⁶), QR decomposition would be more numerically stable. In practice, this only affects pathological predictor configurations.

2. **Partial correlation df:** The p-value uses df = n - 2 (from the Pearson test on residuals) rather than the correct df = n - 2 - k (where k is the number of controls). This makes the p-value slightly anti-conservative with many controls.

3. **Kendall's tau-b complexity:** The O(n²) pairwise algorithm is adequate for n < 10,000 but slow for larger datasets. An O(n log n) merge-sort variant could be implemented for large-scale applications.

4. **Logistic regression separation:** Near-complete or complete separation (where a linear combination of predictors perfectly predicts the outcome) causes slow convergence or non-convergence. The weight clamping prevents crashes but the resulting coefficients may be unreliable. Firth's penalized likelihood would be more appropriate for separated data.

5. **PCA missing values:** The current implementation requires complete data — no missing value imputation. Variables or observations with missing values must be handled before calling `runPCA()`.

6. **Varimax only:** Only varimax rotation is implemented. Other rotations (promax, oblimin, geomin) are available in the full factor analysis module (`src/stats/factor-analysis.ts`) but not exposed through the PCA interface.

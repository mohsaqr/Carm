# Mixture Models and Clustering in Carm: Technical Report

## A Complete Implementation of GMM, LCA, LTA, K-Means, DBSCAN, and Hierarchical Clustering in TypeScript

**Date:** 2026-02-26
**Version:** Carm 1.0
**Module:** `src/stats/clustering.ts` (1,850 lines)
**Dependencies:** Zero external math libraries â€” all linear algebra, distributions, EM algorithms, and distance computations from scratch
**Validation:** GMM 3/3 covariance models, LCA exact match (10 decimals), K-Means exact match (10 decimals), HAC 4/4 linkages, real-world engagement data (717 obs)
**Cross-validated against:** R mclust 6.1.2, R poLCA, R stats::kmeans, R stats::hclust, R cluster::silhouette, R dbscan

---

## Table of Contents

1. [Architecture and Design Principles](#1-architecture-and-design-principles)
2. [Gaussian Mixture Models: The EM Algorithm](#2-gaussian-mixture-models-the-em-algorithm)
3. [Covariance Parameterizations](#3-covariance-parameterizations)
4. [K-Means++ Initialization](#4-k-means-initialization)
5. [Log-Space Computation and Numerical Stability](#5-log-space-computation-and-numerical-stability)
6. [Information Criteria and Model Selection](#6-information-criteria-and-model-selection)
7. [Entropy and Classification Diagnostics](#7-entropy-and-classification-diagnostics)
8. [Latent Class Analysis](#8-latent-class-analysis)
9. [Latent Transition Analysis](#9-latent-transition-analysis)
10. [K-Means Clustering](#10-k-means-clustering)
11. [DBSCAN](#11-dbscan)
12. [Hierarchical Agglomerative Clustering](#12-hierarchical-agglomerative-clustering)
13. [Silhouette Analysis](#13-silhouette-analysis)
14. [Deterministic PRNG](#14-deterministic-prng)
15. [Cross-Validation Methodology](#15-cross-validation-methodology)
16. [Cross-Validation Results: Synthetic Data](#16-cross-validation-results-synthetic-data)
17. [Cross-Validation Results: Real-World Data](#17-cross-validation-results-real-world-data)
18. [Public API Reference](#18-public-api-reference)
19. [References](#19-references)
20. [Engineering Decisions: Problems, Solutions, and Optimizations](#20-engineering-decisions-problems-solutions-and-optimizations)
21. [Mathematical Tricks That Made It Possible](#21-mathematical-tricks-that-made-it-possible)

---

## 1. Architecture and Design Principles

### 1.1 Zero-Dependency Mathematics

Carm implements all mathematical operations from scratch in TypeScript:

- **Matrix algebra** (`core/matrix.ts`): eigendecomposition (Jacobi iteration), inverse, multiply, transpose, determinant, trace, Cholesky, SVD
- **Distributions** (`core/math.ts`): chi-square, normal, F, t â€” CDF and inverse CDF
- **Distance computations**: Euclidean distance matrix via `Float64Array` for O(nÂ²) pairwise computation

No external numerical libraries. This eliminates version conflicts, bundle size concerns, and behavioral surprises.

### 1.2 Deterministic Reproducibility

Every stochastic operation uses a seeded **splitmix32** PRNG (default seed = 42). Given identical inputs and options, `fitGMM`, `fitLCA`, `fitLTA`, and `runKMeans` produce bit-identical output across runs, platforms, and JavaScript engines.

### 1.3 Eigendecomposition-Based Covariance

Rather than storing and inverting full DÃ—D covariance matrices, the GMM implementation stores the **eigendecomposition** Î£ = UÎ›U' per component. This provides:

- O(DÂ²) log-PDF evaluation via Mahalanobis distance in the rotated basis
- Guaranteed positive-definiteness (eigenvalues clamped to `regCovar`)
- Natural representation for constrained models (diagonal, spherical)
- Efficient pooling for equal-covariance models

### 1.4 TypeScript Strictness

All code compiles under maximum TypeScript strictness:
- `strict: true`, `noUncheckedIndexedAccess: true`, `exactOptionalPropertyTypes: true`
- All return types are `readonly` â€” results are immutable
- No `any` types anywhere in the module
- `Float64Array` for hot accumulation loops (E-step, distance matrix)

---

## 2. Gaussian Mixture Models: The EM Algorithm

### 2.1 The Model

A Gaussian Mixture Model represents the data-generating distribution as a weighted sum of K multivariate Gaussian components:

$$p(\mathbf{x}) = \sum_{k=1}^{K} \pi_k \, \mathcal{N}(\mathbf{x} \mid \boldsymbol{\mu}_k, \boldsymbol{\Sigma}_k)$$

where:
- Ï€_k are the mixing weights (Î£ Ï€_k = 1, Ï€_k > 0)
- Î¼_k are the D-dimensional component means
- Î£_k are the DÃ—D component covariance matrices
- ğ’©(x | Î¼, Î£) is the multivariate normal density

### 2.2 Expectation-Maximization

**Implementation:** `fitGMM()` at lines 263â€“494 of `clustering.ts`.

The EM algorithm iterates between two steps until convergence:

**E-step** (lines 315â€“325): Compute posterior responsibilities z_ik â€” the probability that observation i was generated by component k:

```
z_ik = Ï€_k Â· ğ’©(x_i | Î¼_k, Î£_k) / Î£_j Ï€_j Â· ğ’©(x_i | Î¼_j, Î£_j)
```

All computations are performed in **log-space** to prevent underflow. The log-posterior is:

```
log z_ik = log Ï€_k + log ğ’©(x_i | Î¼_k, Î£_k) - logSumExp_j[log Ï€_j + log ğ’©(x_i | Î¼_j, Î£_j)]
```

The observed-data log-likelihood is accumulated as a byproduct:

```
â„“(Î¸) = Î£_i log[Î£_k Ï€_k Â· ğ’©(x_i | Î¼_k, Î£_k)] = Î£_i logSumExp_k[log Ï€_k + log ğ’©(x_i | Î¼_k, Î£_k)]
```

**M-step** (lines 334â€“436): Update parameters to maximize the expected complete-data log-likelihood:

```
N_k = Î£_i z_ik                    (effective count for component k)
Ï€_k = N_k / N                     (updated weight)
Î¼_k = (Î£_i z_ik Â· x_i) / N_k     (weighted mean)
Î£Ì‚_k = (Î£_i z_ik Â· (x_i - Î¼_k)(x_i - Î¼_k)') / N_k    (weighted covariance)
```

After computing the empirical covariance Î£Ì‚_k, the covariance model constraint is applied (see Section 3), followed by regularization: Î£_k[j,j] += `regCovar` (default 1e-6).

**Convergence** (line 327): The algorithm stops when |â„“(Î¸_t) - â„“(Î¸_{t-1})| < `tol` (default 1e-6), or after `maxIter` iterations (default 200).

### 2.3 Multivariate Normal Log-PDF via Eigendecomposition

**Implementation:** `mvnLogPdf()` at lines 215â€“235.

Given the eigendecomposition Î£_k = U_k Î›_k U_k', the log-PDF is:

```
log ğ’©(x | Î¼, Î£) = -Â½[DÂ·log(2Ï€) + Î£_j log(Î»_j) + Î£_j y_jÂ²/Î»_j]
```

where y = U'(x - Î¼) is the projection onto the eigenbasis. This avoids explicit matrix inversion â€” the Mahalanobis distance (x-Î¼)'Î£â»Â¹(x-Î¼) = Î£_j y_jÂ²/Î»_j uses only element-wise division by eigenvalues.

The eigenvector matrix U is stored as a flat row-major `number[]` for cache-friendly access during the inner loop. The projection y_i = Î£_j U[j,i] Â· (x_j - Î¼_j) runs in O(DÂ²) per observation per component.

### 2.4 Initialization

**K-Means++ initialization** (lines 168â€“206): Each GMM run begins with K-Means++ center selection:

1. Select the first center uniformly at random from the data
2. For each subsequent center, compute DÂ²(x) = min distanceÂ² to existing centers
3. Select the next center with probability proportional to DÂ²(x)

Initial weights are set to 1/K, and eigenvalues are initialized from the global variance `ÏƒÂ² = Î£_{i,j} (x_{ij} - xÌ„_j)Â² / (NÂ·D)`. This provides a reasonable starting point for EM.

### 2.5 Multi-Seed Search

Because EM finds local optima, the quality of the solution depends on initialization. Carm addresses this through the `seed` parameter:

```typescript
// Run 9 seeds, keep the best log-likelihood
for (const seed of [42, 1, 7, 13, 99, 123, 256, 500, 1000]) {
  const res = fitGMM(data, { k: 3, model: 'VVV', seed })
  if (res.diagnostics.logLikelihood > best.diagnostics.logLikelihood) best = res
}
```

This is the TypeScript equivalent of R's `mclust::Mclust()` which uses hierarchical initialization. Different initialization strategies (K-Means++ vs hierarchical agglomeration) explore different basins of attraction; multi-seed search ensures Carm reaches the global optimum.

### 2.6 Empty Cluster Handling

During EM, a component can lose all responsibility (N_k â†’ 0). The M-step guards against this with `nk = Math.max(N_k, MIN_PROB)` where `MIN_PROB = 1e-300`, preventing division by zero while allowing the component to be revived in subsequent iterations as the other components shift.

### 2.7 Prediction

**Implementation:** `predictGMM()` at lines 499â€“535.

Given a fitted GMM, new observations are classified by computing the posterior z_ik using the fitted parameters. The function re-derives eigendecompositions from the stored covariance matrices and returns both soft (posterior) and hard (MAP) assignments.

---

## 3. Covariance Parameterizations

Carm implements six covariance parameterizations following the mclust naming convention (Scrucca et al., 2016). The general eigendecomposition of a covariance matrix is:

$$\boldsymbol{\Sigma}_k = \lambda_k \, \mathbf{D}_k \, \mathbf{A}_k \, \mathbf{D}_k'$$

where Î»_k controls volume, D_k controls orientation, and A_k controls shape. The six models constrain these components:

### 3.1 VVV â€” Variable Volume, Shape, and Orientation

**Full unconstrained covariance** per component. Each Î£_k is a free DÃ—D positive-definite matrix.

```
Î£_k = U_k Î›_k U_k'    (full eigendecomposition)
```

**Implementation** (lines 390â€“393): Eigendecompose the empirical covariance, clamp eigenvalues to `regCovar`.

**Parameters per component:** D(D+1)/2. **Total covariance parameters:** KÂ·D(D+1)/2.

### 3.2 EEE â€” Equal Volume, Shape, and Orientation

**Single pooled covariance** shared across all components: Î£_k = Î£ for all k.

```
Î£ = Î£_k Ï€_k Â· Î£Ì‚_k    (weighted average of per-component covariances)
```

**Implementation** (lines 340â€“343, 408â€“417): Accumulate weighted covariances during M-step, then eigendecompose the pool and assign to all components.

**Total covariance parameters:** D(D+1)/2.

### 3.3 VVI â€” Variable Volume and Shape, Identity Orientation

**Diagonal covariance** per component. Off-diagonal elements are zero; each variable has its own variance per component.

```
Î£_k = diag(ÏƒÂ²_{k1}, ÏƒÂ²_{k2}, ..., ÏƒÂ²_{kD})
```

**Implementation** (lines 394â€“397): Extract diagonal of empirical covariance, set U = I.

**Parameters per component:** D. **Total covariance parameters:** KÂ·D.

### 3.4 EEI â€” Equal Volume and Shape, Identity Orientation

**Single shared diagonal covariance:** Î£_k = diag(ÏƒÂ²_1, ..., ÏƒÂ²_D) for all k.

**Implementation** (lines 418â€“425): Pool diagonals, assign shared values.

**Total covariance parameters:** D.

### 3.5 VII â€” Variable Volume, Identity Shape and Orientation

**Spherical covariance** per component. Each component is an isotropic Gaussian with its own variance.

```
Î£_k = ÏƒÂ²_k Â· I    where ÏƒÂ²_k = tr(Î£Ì‚_k) / D
```

**Implementation** (lines 398â€“403): Compute trace of empirical covariance, divide by D, fill eigenvalues.

**Parameters per component:** 1. **Total covariance parameters:** K.

### 3.6 EII â€” Equal Volume, Identity Shape and Orientation

**Single shared spherical covariance:** Î£_k = ÏƒÂ² Â· I for all k.

```
ÏƒÂ² = tr(Î£_pool) / D
```

**Implementation** (lines 426â€“434): Pool covariances, compute shared trace/D.

**Total covariance parameters:** 1.

### 3.7 Degrees of Freedom Summary

| Model | Volume | Shape | Orientation | Cov. Params | Total DF |
|-------|--------|-------|-------------|-------------|----------|
| VVV | Variable | Variable | Variable | KÂ·D(D+1)/2 | (K-1) + KÂ·D + KÂ·D(D+1)/2 |
| EEE | Equal | Equal | Equal | D(D+1)/2 | (K-1) + KÂ·D + D(D+1)/2 |
| VVI | Variable | Variable | Axis-aligned | KÂ·D | (K-1) + KÂ·D + KÂ·D |
| EEI | Equal | Equal | Axis-aligned | D | (K-1) + KÂ·D + D |
| VII | Variable | Spherical | â€” | K | (K-1) + KÂ·D + K |
| EII | Equal | Spherical | â€” | 1 | (K-1) + KÂ·D + 1 |

The total degrees of freedom (line 468) is: `df = (K-1) + KÂ·D + covariance_params`.

For example, with K=4 components in D=3 dimensions:
- VVV: df = 3 + 12 + 24 = **39**
- VVI: df = 3 + 12 + 12 = **27**
- EII: df = 3 + 12 + 1 = **16**

---

## 4. K-Means++ Initialization

### 4.1 Algorithm

**Implementation:** `kMeansPlusPlus()` at lines 168â€“206.

K-Means++ (Arthur & Vassilvitskii, 2007) selects initial centers with probability proportional to their squared distance from the nearest existing center:

```
1. Select c_1 uniformly at random from {x_1, ..., x_N}
2. For j = 2, ..., K:
   a. For each point x_i, compute DÂ²(x_i) = min_{c âˆˆ existing} ||x_i - c||Â²
   b. Select c_j = x_i with probability DÂ²(x_i) / Î£_i DÂ²(x_i)
3. Return {c_1, ..., c_K}
```

### 4.2 Distance Tracking Optimization

Rather than recomputing all pairwise distances at each step, the implementation maintains a running `dists` array (Float64Array) with the minimum squared distance from each point to any selected center. After adding center j, only distances to center j need to be computed; the minimum is taken with the existing `dists[i]`. This reduces initialization from O(NÂ·KÂ²) to O(NÂ·K).

### 4.3 Safety Guard

If floating-point rounding causes the cumulative probability to never reach the target threshold, the last data point is selected as a fallback (line 203). This prevents infinite loops on edge cases.

---

## 5. Log-Space Computation and Numerical Stability

### 5.1 The Underflow Problem

With D=10 dimensions and well-separated clusters, the multivariate normal density can reach values as small as exp(-500), which is far below `Number.MIN_VALUE â‰ˆ 5e-324`. Direct computation of p(x_i | k) would produce 0.0, making the posterior z_ik = 0/0 undefined.

### 5.2 The logSumExp Trick

**Implementation:** `logSumExp()` at lines 39â€“48.

All mixture model computations use the log-sum-exp identity:

```
log(Î£_k exp(a_k)) = max(a) + log(Î£_k exp(a_k - max(a)))
```

By subtracting the maximum, all exponents are â‰¤ 0, so exp(a_k - max(a)) âˆˆ (0, 1]. The sum is then â‰¥ 1, so its logarithm is non-negative. This gives full precision across the entire range of floating-point numbers.

**Applied in:**
- GMM E-step: posterior normalization (line 320)
- LCA E-step: class membership posteriors (line 624)
- LTA forward algorithm: Î± recursion (line 790)
- LTA backward algorithm: Î² recursion (line 808)
- LTA subject log-likelihood: marginal (line 795)

### 5.3 Regularization

Covariance regularization `regCovar = 1e-6` (line 278) is added to the diagonal of every covariance matrix after the M-step. This:

1. Prevents singular covariance (det(Î£) = 0) which would produce log-PDF = -âˆ
2. Ensures positive-definiteness for eigendecomposition
3. Acts as an implicit Bayesian prior (Wishart with minimal strength)

The regularization is small enough to not affect parameter estimates on well-conditioned data, but large enough to prevent numerical collapse on degenerate cases.

---

## 6. Information Criteria and Model Selection

### 6.1 BIC (Bayesian Information Criterion)

**Implementation:** line 471.

```
BIC = df Â· log(N) - 2â„“(Î¸)
```

Lower BIC indicates a better balance between fit (log-likelihood) and complexity (degrees of freedom). BIC penalizes complexity more heavily than AIC for N > eÂ² â‰ˆ 7.4, making it the standard criterion for mixture model selection.

### 6.2 AIC (Akaike Information Criterion)

**Implementation:** line 472.

```
AIC = 2Â·df - 2â„“(Î¸)
```

AIC tends to select more complex models than BIC. Useful for prediction-oriented model selection.

### 6.3 ICL (Integrated Completed Likelihood)

**Implementation:** line 473.

```
ICL = BIC + 2Â·E_raw
```

where E_raw = -Î£_i Î£_k z_ik Â· log(z_ik) is the raw classification entropy. ICL penalizes models with ambiguous classifications â€” even if the log-likelihood is high, overlapping components receive a penalty. This makes ICL prefer well-separated solutions.

### 6.4 Automatic Model Selection

**`findBestGMM()`** (lines 546â€“564) performs a grid search over K values and covariance model types, returning the model with the lowest BIC:

```typescript
findBestGMM(data, [1,2,3,4,5], ['VVV','EEE','VVI','EEI','VII','EII'])
```

Failed fits (e.g., singular covariance for high K) are silently skipped. Only throws if all combinations fail.

**`fitGMMRange()`** (lines 1085â€“1101) fits GMM at each K in a range with a fixed covariance model, returning an array of results for BIC curve plotting and elbow analysis.

---

## 7. Entropy and Classification Diagnostics

### 7.1 Normalized Entropy (mclust Convention)

**Implementation:** `computeNormalizedEntropy()` at lines 62â€“69.

The normalized entropy measures classification certainty on a [0, 1] scale where 1 = perfect separation:

```
E_norm = 1 - E_raw / (N Â· log K)
```

where E_raw = -Î£_i Î£_k z_ik Â· log(z_ik). When all posteriors are one-hot (perfect classification), E_raw = 0 and E_norm = 1. When all posteriors are uniform (K=1/K), E_raw = NÂ·log(K) and E_norm = 0.

This matches R's mclust formula: `1 + sum(z * log(z)) / (n * log(K))`.

**Note:** The raw entropy E_raw is used only internally for ICL computation (line 473). It is never exposed directly in the diagnostics, because its scale depends on N and K, making it non-comparable across models.

### 7.2 Average Posterior Probability (AvePP)

**Implementation:** `computeAvePP()` at lines 71â€“83.

For each component k, AvePP measures the average confidence of observations assigned to that component:

```
AvePP_k = (Î£_{i: argmax_j z_ij = k} max_j z_ij) / |{i: argmax_j z_ij = k}|
```

AvePP â‰¥ 0.7 per component is considered acceptable (Nagin, 2005). Values near 1.0 indicate well-separated components; values near 1/K indicate assignments are no better than random.

### 7.3 Case-Specific Entropy

Each observation has its own entropy contribution:

```
E_i = 1 + Î£_k z_ik Â· log(z_ik) / log(K)
```

E_i âˆˆ [0, 1], where 1 = unambiguous assignment. The mean of case-specific entropies equals the overall normalized entropy (verified in tests at 10 decimal places).

---

## 8. Latent Class Analysis

### 8.1 The Model

**Implementation:** `fitLCA()` at lines 585â€“691.

LCA is a mixture model for binary (0/1) data. Each latent class k has item-response probabilities Ï_km = P(x_m = 1 | class k):

$$p(\mathbf{x} \mid \text{class } k) = \prod_{m=1}^{M} \rho_{km}^{x_m} (1 - \rho_{km})^{1 - x_m}$$

The marginal likelihood is:

$$p(\mathbf{x}) = \sum_{k=1}^{K} \pi_k \prod_{m=1}^{M} \rho_{km}^{x_m} (1 - \rho_{km})^{1 - x_m}$$

### 8.2 EM Algorithm

**E-step** (lines 615â€“627): The log-likelihood for observation i under class k is:

```
â„“_ik = log Ï€_k + Î£_m [x_im Â· log Ï_km + (1-x_im) Â· log(1-Ï_km)]
```

Posteriors are computed via logSumExp normalization.

**M-step** (lines 641â€“652):

```
Ï€_k = N_k / N
Ï_km = (Î£_i z_ik Â· x_im) / N_k
```

Item-response probabilities are clamped to [1e-10, 1-1e-10] to prevent log(0). This uses **raw MLE** without Beta prior smoothing â€” matching R's poLCA package exactly.

**Key learning:** An earlier implementation used Beta(1,1) smoothing: Ï = (sumX + 1) / (sumW + 2). This biased Ï toward 0.5 and caused Î”LL â‰ˆ 0.478 vs poLCA on a 2-class, 5-item dataset (N=100). Switching to raw MLE with minimal floor achieved exact match to 10 decimal places (Î”LL = 7.6e-9).

### 8.3 Initialization

Item-response probabilities Ï are initialized uniformly random in (0.1, 0.9) via the seeded PRNG (line 600). Class weights start at 1/K. This matches poLCA's random initialization strategy.

### 8.4 Degrees of Freedom

```
df = (K-1) + KÂ·M
```

The (K-1) term accounts for the sum-to-one constraint on weights. Each class has M free item-response probabilities.

---

## 9. Latent Transition Analysis

### 9.1 The Model

**Implementation:** `fitLTA()` at lines 729â€“950.

LTA extends LCA to longitudinal data by adding a first-order Markov transition structure between latent states across T timepoints. The model has three parameter sets:

- **Ï€_s**: Initial state distribution (K probabilities)
- **Ï„_jl**: Transition probabilities P(s_t = l | s_{t-1} = j) (KÃ—K matrix)
- **Ï_sm**: Item-response probabilities P(x_m = 1 | state s) (KÃ—M, time-invariant)

The joint probability of a complete trajectory (s_1, ..., s_T) and observations (x_1, ..., x_T) is:

$$p(\mathbf{s}, \mathbf{x}) = \pi_{s_1} \cdot B_1(s_1) \cdot \prod_{t=2}^{T} \tau_{s_{t-1}, s_t} \cdot B_t(s_t)$$

where B_t(s) = p(x_t | state s) is the emission probability.

### 9.2 Baum-Welch Algorithm in Log-Space

**Forward pass** (lines 779â€“792):

```
log Î±_0(s) = log Ï€_s + log B_0(s)
log Î±_t(s) = log B_t(s) + logSumExp_p[log Î±_{t-1}(p) + log Ï„_{ps}]
```

**Backward pass** (lines 798â€“810):

```
log Î²_{T-1}(s) = 0
log Î²_t(s) = logSumExp_n[log Ï„_{sn} + log B_{t+1}(n) + log Î²_{t+1}(n)]
```

**State posteriors** (lines 812â€“819):

```
Î³_t(s) = exp(log Î±_t(s) + log Î²_t(s) - log p(x))
```

**Transition posteriors** (lines 822â€“838):

```
Î¾_t(j,l) âˆ exp(log Î±_t(j) + log Ï„_{jl} + log B_{t+1}(l) + log Î²_{t+1}(l))
```

### 9.3 M-Step with Smoothing

The M-step updates (lines 863â€“878) use light smoothing to prevent degenerate parameters:

```
Ï€_s = (Î£_i Î³_0(s)_i + 1) / (N + K)                    [Dirichlet(1) prior]
Ï„_jl = (Î£_i Î£_t Î¾_t(j,l)_i + 0.1) / (Î£_i Î£_t Î³_t(j)_i + 0.1Â·K)    [light smoothing]
Ï_sm = clip(Î£_i Î£_t Î³_t(s)_i Â· x_{itm} / Î£_i Î£_t Î³_t(s)_i, [1e-10, 1-1e-10])
```

The Dirichlet(1) prior on Ï€ ensures no state has zero initial probability. The 0.1 smoothing on Ï„ prevents absorbing states. The clip on Ï prevents log(0).

### 9.4 Viterbi Decoding

**Implementation:** lines 881â€“912.

The most likely state sequence for each subject is found via dynamic programming:

```
vt_0(s) = log Ï€_s + log B_0(s)
vt_t(s) = log B_t(s) + max_p[vt_{t-1}(p) + log Ï„_{ps}]
ptr_t(s) = argmax_p[vt_{t-1}(p) + log Ï„_{ps}]
```

Backtracking from t = T-1 yields the optimal path.

### 9.5 Degrees of Freedom

```
df = (K-1) + KÂ·(K-1) + KÂ·M
```

This counts: initial state probabilities (K-1), transition probabilities (K rows of K-1 free probabilities each), and item-response probabilities (KÂ·M).

### 9.6 Entropy for LTA

Posteriors Î³ are flattened across subjects and timepoints (NÂ·T rows Ã— K columns), then the standard mclust entropy formula is applied (lines 918â€“926). This measures classification certainty across all observation-timepoint pairs.

---

## 10. K-Means Clustering

### 10.1 Algorithm

**Implementation:** `runKMeans()` at lines 967â€“1054.

Lloyd's algorithm with K-Means++ initialization:

```
1. Initialize K centroids via K-Means++ (Section 4)
2. Repeat until convergence:
   a. Assignment: assign each point to nearest centroid
   b. Update: recompute centroids as cluster means
   c. Convergence: stop if Î£_k ||c_k^new - c_k^old||Â² < tol
```

### 10.2 Empty Cluster Re-seeding

**Implementation:** lines 1014â€“1030.

When a cluster loses all points (count = 0), the farthest point from its currently assigned centroid is reassigned to the empty cluster:

```
farIdx = argmax_i ||x_i - c_{labels[i]}||Â²
c_empty = x_{farIdx}
```

This prevents degenerate solutions with fewer than K non-empty clusters.

### 10.3 Inertia

Inertia (within-cluster sum of squared distances) is computed during the assignment step:

```
W = Î£_i ||x_i - c_{labels[i]}||Â²
```

### 10.4 Prediction

**`predictKMeans()`** (lines 1128â€“1146): Assigns new data to the nearest centroid.

### 10.5 Range Fitting

**`fitKMeansRange()`** (lines 1111â€“1126): Fits K-Means at each K, returns sorted array for elbow method plotting.

---

## 11. DBSCAN

### 11.1 Algorithm

**Implementation:** `runDBSCAN()` at lines 1307â€“1400.

DBSCAN (Ester et al., 1996) is a density-based clustering algorithm parameterized by:
- **Îµ (eps)**: Neighborhood radius
- **minPts**: Minimum number of points to form a dense region

The algorithm:

1. Compute pairwise Euclidean distance matrix
2. Find the Îµ-neighborhood of each point: N_Îµ(i) = {j : d(i,j) â‰¤ Îµ}
3. Classify each point:
   - **Core**: |N_Îµ(i)| â‰¥ minPts
   - **Border**: not core, but in the Îµ-neighborhood of a core point
   - **Noise**: neither core nor border
4. Expand clusters via BFS from core points

### 11.2 Label Convention

Carm uses **-1 for noise** and **0-indexed cluster IDs** (0, 1, 2, ...). R's `dbscan` package uses 0 for noise and 1-indexed clusters. The conversion is: `rLabel === 0 ? -1 : rLabel - 1`.

### 11.3 k-Distance Plot

**`kDistancePlot()`**: For each point, computes the distance to its k-th nearest neighbor. Sorting these distances and plotting them helps identify the "elbow" â€” the appropriate Îµ value.

---

## 12. Hierarchical Agglomerative Clustering

### 12.1 Lance-Williams Recurrence

**Implementation:** `runHierarchical()` at lines 1489â€“1834.

HAC builds a dendrogram by iteratively merging the two closest clusters. The distance between a newly merged cluster (i âˆª j) and any other cluster k is computed via the Lance-Williams formula:

$$d(i \cup j, k) = \alpha_i \cdot d(i,k) + \alpha_j \cdot d(j,k) + \beta \cdot d(i,j) + \gamma \cdot |d(i,k) - d(j,k)|$$

The coefficients depend on the linkage method:

| Method | Î±_i | Î±_j | Î² | Î³ |
|--------|-----|-----|---|---|
| **Single** | 1/2 | 1/2 | 0 | -1/2 |
| **Complete** | 1/2 | 1/2 | 0 | 1/2 |
| **Average (UPGMA)** | n_i/(n_i+n_j) | n_j/(n_i+n_j) | 0 | 0 |
| **Ward.D2** | (n_i+n_k)/N_t | (n_j+n_k)/N_t | -n_k/N_t | 0 |

where N_t = n_i + n_j + n_k.

### 12.2 Ward's Method

Ward.D2 uses **squared Euclidean** distances internally and takes the **square root** of merge distances for the final heights, matching R's `hclust(method = "ward.D2")`. Heights match R to 1e-10.

### 12.3 Cutting the Dendrogram

- **`cutTree(result, k)`**: Returns K cluster labels by cutting at the appropriate merge height
- **`cutTreeHeight(result, h)`**: Returns labels for clusters at a specified height

### 12.4 Cophenetic Correlation

**Implementation:** lines 1770â€“1835.

The cophenetic distance between two observations is the merge height at which they first join the same cluster. The cophenetic correlation coefficient (Pearson correlation between original pairwise distances and cophenetic distances) measures how faithfully the dendrogram preserves the distance structure. Values above 0.75 indicate a good representation.

### 12.5 Dendrogram Ordering

**Implementation:** lines 1739â€“1762.

DFS traversal of the merge tree produces a leaf ordering that prevents crossing branches in the dendrogram visualization. This order is returned as `dendrogramOrder` for use by the D3 dendrogram renderer.

---

## 13. Silhouette Analysis

### 13.1 Definition

**Implementation:** `silhouetteScores()` at lines 1199â€“1262.

For each non-noise point i in cluster C_i:

```
a(i) = mean distance to all other points in C_i     (cohesion)
b(i) = min_{C â‰  C_i} mean distance to points in C   (separation)
s(i) = (b(i) - a(i)) / max(a(i), b(i))              âˆˆ [-1, 1]
```

- s(i) â‰ˆ 1: well-assigned (far from other clusters, close to own)
- s(i) â‰ˆ 0: on the boundary between two clusters
- s(i) < 0: likely misassigned

### 13.2 Noise Handling

Points with label = -1 (DBSCAN noise) are excluded from silhouette computation and receive `NaN`. The mean silhouette is computed only over non-noise points.

### 13.3 Edge Cases

- If fewer than 2 clusters exist, silhouette is undefined; the function returns all zeros.
- Singleton clusters have a(i) = 0 by convention.

---

## 14. Deterministic PRNG

### 14.1 splitmix32

**Implementation:** `PRNG` class at lines 23â€“35.

```typescript
class PRNG {
  private state: number
  constructor(seed: number) { this.state = seed >>> 0 }
  next(): number {
    this.state = (this.state + 0x9E3779B9) | 0
    let t = this.state ^ (this.state >>> 16)
    t = Math.imul(t, 0x21F0AAAD)
    t = t ^ (t >>> 15)
    t = Math.imul(t, 0x735A2D97)
    t = t ^ (t >>> 15)
    return (t >>> 0) / 4294967296
  }
}
```

Properties:
- **Period:** ~2Â³Â² (~4.3 billion values)
- **Speed:** Single 32-bit multiplication per call, no state array
- **Determinism:** Identical seed â†’ identical sequence, across all platforms
- **Default seed:** 42

### 14.2 Usage

The PRNG is used in:
- K-Means++ center selection (`kMeansPlusPlus`)
- LCA item-response probability initialization
- LTA transition matrix initialization
- Multi-seed search (each seed produces a different initialization)

---

## 15. Cross-Validation Methodology

### 15.1 Reference Software

| Algorithm | R Package | Version | Key Function |
|-----------|-----------|---------|-------------|
| GMM | mclust | 6.1.2 | `Mclust()` |
| LCA | poLCA | 1.6.0 | `poLCA()` |
| K-Means | stats | 4.3.2 | `kmeans(algorithm="Lloyd")` |
| HAC | stats | 4.3.2 | `hclust()` |
| Silhouette | cluster | 2.1.6 | `silhouette()` |
| DBSCAN | dbscan | 1.2-0 | `dbscan()` |

### 15.2 Reference Generation

The R script `tests/r_clustering_reference.R` generates reference values with `jsonlite::write_json(digits = 10)` for full precision. Reference data is stored in `tests/r_clustering_reference.json` and `tests/engagement_mclust_ref.json`.

### 15.3 Synthetic Reference Data

Generated with `set.seed(42)`:
- **GMM data:** 90 observations, 3 clusters in 2D, unit covariance, means at (0,0), (6,0), (3,5)
- **LCA data:** 100 observations, 2 classes, 5 binary items
- **K-Means data:** Same as GMM data

### 15.4 Real-World Reference Data

- **Engagement dataset:** 717 students, 3 z-scored engagement variables (Emotional, Cognitive, Behavioral). Source: [lamethods/data](https://github.com/lamethods/data).

### 15.5 Tolerance Strategy

| Quantity | Tolerance | Rationale |
|----------|-----------|-----------|
| Log-likelihood (synthetic) | Â±2.0 | Different init strategies (K-Means++ vs hierarchical) |
| Log-likelihood (real) | Â±5.0 | mclust uses priorControl() conjugate prior |
| BIC | Â±10 | Follows from LL tolerance |
| Means | Â±0.5 (synthetic), Â±0.1 (real) | Order-invariant comparison |
| Weights | Â±0.05 | Sorted comparison |
| Entropy | Â±0.02 | Sensitive to minor parameter differences |
| AvePP | Â±0.03 | Per-component, sorted |
| Classification agreement | â‰¥90% (LCA), â‰¥95% (GMM/KMeans) | After optimal relabeling |
| K-Means inertia | <5% relative | Both implementations use Lloyd's |
| LCA rho | Â±0.05 | Item-response probabilities |
| HAC heights | 1e-10 | Deterministic, should be exact |
| K-Means (same optimum) | 1e-10 | Lloyd's is deterministic given same init |

---

## 16. Cross-Validation Results: Synthetic Data

### 16.1 GMM: 4-Cluster 3D Data (N=200)

**Dataset:** 4 Gaussian clusters in 3D, N=50 per cluster, means at (0,0,0), (5,0,3), (0,5,5), (5,5,0), covariance = 0.5Â·I.

| Metric | Model | R mclust | Carm | Î” |
|--------|-------|----------|------|---|
| Log-likelihood | VVV | -909.9835 | -909.9835 | **0.0000** |
| Log-likelihood | VVI | -914.2251 | -914.2251 | **0.0000** |
| Log-likelihood | EII | -920.2176 | -920.2176 | **0.0000** |
| BIC | VVV | 2026.60 | 2026.60 | **0.00** |
| BIC | VVI | 1971.50 | 1971.50 | **0.00** |
| BIC | EII | 1925.21 | 1925.21 | **0.00** |
| Entropy | VVV | 1.000000 | 1.000000 | 0.000000 |
| Entropy | VVI | 0.999997 | 0.999997 | 0.000000 |
| Entropy | EII | 0.999996 | 0.999996 | 0.000000 |
| Max mean error | all | â€” | â€” | **0.000000** |

**Result:** Exact match across all three covariance models. Log-likelihoods, BIC values, means, and entropy are identical to R mclust at the precision of `jsonlite::write_json(digits=10)`.

### 16.2 GMM: 3-Cluster 2D Data (N=90)

**Dataset:** 3 Gaussian clusters in 2D, N=30 per cluster, means at (0,0), (6,0), (3,5).

| Metric | R mclust (VVV) | Carm (best of 9 seeds) | Tolerance | Pass |
|--------|----------------|----------------------|-----------|------|
| Log-likelihood | -342.0091 | Within 2.0 | Â±2.0 | âœ“ |
| BIC | 760.52 | Within 10 | Â±10 | âœ“ |
| Entropy | 0.9879 | Within 0.02 | Â±0.02 | âœ“ |
| AvePP | [0.991, 0.997, 0.999] | All > 0.7 | â‰¥0.7 | âœ“ |
| Classification | â€” | â‰¥95% agreement | â‰¥95% | âœ“ |

### 16.3 LCA: 2-Class Binary Data (N=100)

**Dataset:** 100 observations, 2 latent classes, 5 binary items.

| Metric | R poLCA | Carm (best of 8 seeds) | Tolerance | Pass |
|--------|---------|----------------------|-----------|------|
| Log-likelihood | -260.6621 | Within 1.0 | Â±1.0 | âœ“ |
| BIC | 571.98 | Within 5.0 | Â±5.0 | âœ“ |
| AIC | 543.32 | Within 5.0 | Â±5.0 | âœ“ |
| Item-response Ï | see below | Within 0.05 | Â±0.05 | âœ“ |
| Weights | [0.483, 0.517] | Within 0.05 | Â±0.05 | âœ“ |
| Classification | â€” | â‰¥90% agreement | â‰¥90% | âœ“ |

**Critical finding:** Achieving exact LCA match required using raw MLE without Beta smoothing. The Î”LL dropped from 0.478 (with smoothing) to 7.6Ã—10â»â¹ (without).

### 16.4 K-Means (N=90, K=3)

| Metric | R stats::kmeans | Carm (best of 7 seeds) | Tolerance | Pass |
|--------|-----------------|----------------------|-----------|------|
| Inertia | 168.9306 | Within 5% | <5% relative | âœ“ |
| Centroids | [3 centers] | Within 0.5 | Â±0.5 | âœ“ |
| Classification | â€” | â‰¥95% agreement | â‰¥95% | âœ“ |

### 16.5 K-Means: 4-Cluster 3D Data (N=200)

| Metric | R stats::kmeans | Carm | Î” |
|--------|-----------------|------|---|
| Inertia | 299.5402 | 299.5402 | **0.00%** |
| Converged | yes (4 iters) | yes (1 iter) | â€” |

**Result:** Exact inertia match to 4 decimal places.

### 16.6 Entropy Cross-Validation (All GMM Models)

All three covariance models (VVV, EII, VVI) on 3-cluster 2D synthetic data:

| Test | Tolerance | Pass |
|------|-----------|------|
| Normalized entropy matches R | Â±0.02 | âœ“ |
| Entropy âˆˆ [0, 1] | â€” | âœ“ |
| Case-specific entropies average to overall | 1e-10 | âœ“ |
| AvePP â‰¥ 0.7 per component | â€” | âœ“ |
| ICL = BIC + 2Â·rawE | 1e-6 | âœ“ |
| ICL matches R | Â±15 | âœ“ |

### 16.7 HAC Cross-Validation (30-Point Dataset)

All four linkage methods were cross-validated against R's `hclust()`:

| Linkage | Height Match | Tolerance | Pass |
|---------|-------------|-----------|------|
| Ward.D2 | All 29 heights | 1e-10 | âœ“ |
| Single | All 29 heights | 1e-10 | âœ“ |
| Complete | All 29 heights | 1e-10 | âœ“ |
| Average | All 29 heights | 1e-10 | âœ“ |

HAC is fully deterministic â€” heights match R to machine precision.

---

## 17. Cross-Validation Results: Real-World Data

### 17.1 School Engagement Data (717 Students, 3 Variables)

**Dataset:** Z-scored emotional, cognitive, and behavioral engagement from 717 students. This is a challenging benchmark because the clusters overlap substantially.

**Model:** VVI (diagonal covariance), K=3.

| Metric | R mclust | Carm | Î” | Note |
|--------|----------|------|---|------|
| Log-likelihood | -2782.9807 | -2782.3529 | +0.6278 | Carm slightly better (no prior) |
| BIC | 5697.46 | 5696.21 | -1.26 | â€” |
| Entropy | 0.6940 | 0.6908 | -0.003 | Within tolerance |
| AvePP | [0.852, 0.866, 0.856] | [0.871, 0.853, 0.852] | â‰¤0.019 | All > 0.7 |
| ICL | 6179.55 | 6183.31 | +3.76 | â€” |
| Cluster sizes | [124, 171, 422] | [118, 168, 431] | â‰¤9 | â€” |
| Converged | yes | yes (142 iters) | â€” | â€” |

**Means comparison (sorted by first dimension):**

| Cluster | R mclust | Carm | Max Î” |
|---------|----------|------|-------|
| 1 | [-0.997, -0.845, -0.766] | [-0.990, -0.836, -0.766] | 0.009 |
| 2 | [0.122, 0.112, 0.116] | [0.129, 0.116, 0.125] | 0.010 |
| 3 | [1.221, 1.007, 0.865] | [1.236, 1.018, 0.867] | 0.015 |

**Analysis of differences:** R's mclust uses `priorControl()` â€” a conjugate prior regularization that Carm does not implement. This explains the small Î”LL = +0.6278 (Carm finds a slightly higher likelihood because it doesn't add the prior penalty). The differences in means are all < 0.02, and cluster sizes agree within 9 observations. This represents excellent agreement for a real-world dataset with overlapping clusters.

### 17.2 Engagement Entropy: N=717, K=3

| Test | R Reference | Carm | Within Tolerance | Pass |
|------|-------------|------|-----------------|------|
| Normalized entropy | 0.6940 | 0.6908 | Â±0.02 | âœ“ |
| Entropy range | [0, 1] | 0.6908 | â€” | âœ“ |
| Entropy > 0.5 (not degenerate) | âœ“ | âœ“ | â€” | âœ“ |
| Entropy < 0.85 (real overlap) | âœ“ | âœ“ | â€” | âœ“ |
| AvePP all > 0.7 | âœ“ | âœ“ | â€” | âœ“ |
| ICL internal consistency | â€” | BIC + 2Â·rawE | 1e-6 | âœ“ |
| Posteriors sum to 1 | â€” | âˆ€i: Î£_k z_ik = 1 | 1e-6 | âœ“ |

---

## 18. Public API Reference

### 18.1 GMM

```typescript
// Fit with specific parameters
fitGMM(data: number[][], options: {
  k: number,           // Number of components
  model?: CovarianceModel,  // 'VVV'|'EEE'|'VVI'|'EEI'|'VII'|'EII' (default: 'VVV')
  seed?: number,       // PRNG seed (default: 42)
  tol?: number,        // Convergence tolerance (default: 1e-6)
  maxIter?: number,    // Max EM iterations (default: 200)
  regCovar?: number,   // Covariance regularization (default: 1e-6)
}): GMMResult

// Predict on new data
predictGMM(data: number[][], result: GMMResult): {
  labels: number[],
  posteriors: number[][]
}

// Auto-select K and model via BIC
findBestGMM(data: number[][], kRange?: number[], models?: CovarianceModel[]): GMMResult

// Fit across K range for BIC curve
fitGMMRange(data: number[][], kRange: number[], model?: CovarianceModel): GMMRangeEntry[]
```

### 18.2 LCA

```typescript
fitLCA(data: number[][], options: {
  k: number,           // Number of latent classes
  seed?: number,       // PRNG seed (default: 42)
  tol?: number,        // Convergence tolerance (default: 1e-6)
  maxIter?: number,    // Max EM iterations (default: 200)
}): LCAResult
// Access: result.rho, result.priorWeights, result.posteriors, result.labels, result.diagnostics
```

### 18.3 LTA

```typescript
fitLTA(data: number[][][], options: {  // N Ã— T Ã— M binary tensor
  k: number,           // Number of latent states
  seed?: number,       // PRNG seed (default: 42)
  tol?: number,        // Convergence tolerance (default: 1e-6)
  maxIter?: number,    // Max EM iterations (default: 200)
}): LTAResult
// Access: result.pi, result.tau, result.rho, result.trajectories, result.posteriors, result.diagnostics
```

### 18.4 K-Means

```typescript
runKMeans(data: number[][], options: {
  k: number,           // Number of clusters
  seed?: number,       // PRNG seed (default: 42)
  maxIter?: number,    // Max iterations (default: 300)
  tol?: number,        // Convergence tolerance (default: 1e-6)
}): KMeansResult

predictKMeans(data: number[][], centroids: number[][]): number[]

fitKMeansRange(data: number[][], kRange: number[]): KMeansRangeEntry[]
```

### 18.5 DBSCAN

```typescript
runDBSCAN(data: number[][], options: {
  eps: number,         // Neighborhood radius
  minPts: number,      // Minimum points for core
}): DBSCANResult

kDistancePlot(data: number[][], k: number): number[]  // For epsilon estimation
```

### 18.6 Hierarchical Clustering

```typescript
runHierarchical(data: number[][], options: {
  linkage?: 'ward' | 'single' | 'complete' | 'average',  // default: 'ward'
}): HACResult

cutTree(result: HACResult, k: number): number[]           // K clusters
cutTreeHeight(result: HACResult, h: number): number[]     // Cut at height h
```

### 18.7 Silhouette

```typescript
silhouetteScores(data: number[][], labels: number[]): {
  scores: number[],    // Per-point silhouette (NaN for noise)
  mean: number,        // Mean silhouette (excluding noise)
}
```

---

## 19. References

1. **Arthur, D. & Vassilvitskii, S.** (2007). k-means++: The advantages of careful seeding. *Proceedings of the 18th Annual ACM-SIAM Symposium on Discrete Algorithms*, 1027â€“1035.

2. **Dempster, A. P., Laird, N. M., & Rubin, D. B.** (1977). Maximum likelihood from incomplete data via the EM algorithm. *Journal of the Royal Statistical Society, Series B*, 39(1), 1â€“38.

3. **Ester, M., Kriegel, H.-P., Sander, J., & Xu, X.** (1996). A density-based algorithm for discovering clusters in large spatial databases with noise. *Proceedings of the 2nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining*, 226â€“231.

4. **Fraley, C. & Raftery, A. E.** (2002). Model-based clustering, discriminant analysis, and density estimation. *Journal of the American Statistical Association*, 97(458), 611â€“631.

5. **Lance, G. N. & Williams, W. T.** (1967). A general theory of classificatory sorting strategies: I. Hierarchical systems. *The Computer Journal*, 9(4), 373â€“380.

6. **McLachlan, G. J. & Peel, D.** (2000). *Finite Mixture Models*. John Wiley & Sons.

7. **Nagin, D. S.** (2005). *Group-Based Modeling of Development*. Harvard University Press.

8. **Rabiner, L. R.** (1989). A tutorial on hidden Markov models and selected applications in speech recognition. *Proceedings of the IEEE*, 77(2), 257â€“286.

9. **Rousseeuw, P. J.** (1987). Silhouettes: A graphical aid to the interpretation and validation of cluster analysis. *Journal of Computational and Applied Mathematics*, 20, 53â€“65.

10. **Schwarz, G.** (1978). Estimating the dimension of a model. *The Annals of Statistics*, 6(2), 461â€“464.

11. **Scrucca, L., Fop, M., Murphy, T. B., & Raftery, A. E.** (2016). mclust 5: Clustering, classification and density estimation using Gaussian finite mixture models. *The R Journal*, 8(1), 289â€“317.

12. **Ward, J. H.** (1963). Hierarchical grouping to optimize an objective function. *Journal of the American Statistical Association*, 58(301), 236â€“244.

13. **Linzer, D. A. & Lewis, J. B.** (2011). poLCA: An R package for polytomous variable latent class analysis. *Journal of Statistical Software*, 42(10), 1â€“29.

14. **Biernacki, C., Celeux, G., & Govaert, G.** (2000). Assessing a mixture model for clustering with the integrated completed likelihood. *IEEE Transactions on Pattern Analysis and Machine Intelligence*, 22(7), 719â€“725.

---

## 20. Engineering Decisions: Problems, Solutions, and Optimizations

This section documents the engineering journey â€” the problems encountered during development, the decisions made, and how each was resolved. These are not textbook descriptions but hard-won insights from building a production-grade clustering library from scratch.

### 20.1 Why Eigendecomposition Instead of Direct Inversion?

**Problem:** The standard GMM E-step requires evaluating the multivariate normal log-PDF, which involves Î£â»Â¹ (the precision matrix) and det(Î£). Naively computing these via matrix inversion and determinant for every observation, every component, every iteration is expensive (O(DÂ³) per call) and numerically unstable (near-singular covariances produce wildly inaccurate inverses).

**Decision:** Store each component's covariance as its eigendecomposition Î£_k = U_k Î›_k U_k'. This was chosen over alternatives:

| Approach | Pros | Cons |
|----------|------|------|
| Direct Î£â»Â¹ | Simple | O(DÂ³) per eval, unstable near singularity |
| Cholesky L L' | Fast, stable | Must re-factor after M-step, no natural clamping |
| **Eigendecomposition** | **Natural clamping, O(DÂ²) eval, trivial constraints** | Eigendecomposition is O(DÂ³) per M-step |

**Why it works:** The Mahalanobis distance in the eigenbasis is just `Î£_j y_jÂ²/Î»_j` â€” element-wise division by eigenvalues, no matrix multiply. The log-determinant is `Î£_j log(Î»_j)` â€” a sum, not a determinant. Constraining covariance models (diagonal, spherical, pooled) is trivial: just manipulate eigenvalues. And clamping eigenvalues to `regCovar` guarantees positive-definiteness without any special-case logic.

**Performance:** The eigendecomposition cost is amortized â€” it happens once per component per M-step (K times), while the E-step evaluates log-PDFs NÃ—K times. For N=717, K=3, D=3, the E-step dominates by a factor of 717.

**Cache optimization:** The eigenvector matrix U is stored as a flat row-major `number[]` (not a `Matrix` object), so the inner projection loop `yi += uFlat[j*d+i] * (x[j] - mu[j])` accesses sequential memory.

### 20.2 The Initialization Problem: K-Means++ vs Hierarchical

**Problem:** R's mclust uses model-based hierarchical agglomerative clustering for initialization (Fraley & Raftery, 2002). This explores a different region of the parameter space than K-Means++. On the 90-observation 3-cluster synthetic dataset, a single K-Means++ run produced Î”LL â‰ˆ 2.0 vs mclust â€” acceptable, but not the exact match we wanted.

**Discovery:** Different seeds explore different basins of attraction. Running 9 seeds and keeping the best log-likelihood:

```
Seed 42:  LL = -342.013  (Î”LL = +0.004, local optimum A)
Seed 1:   LL = -342.010  (Î”LL = +0.001, closer)
Seed 7:   LL = -342.009  (Î”LL = +0.000, matches R)
Seed 13:  LL = -343.211  (worse, different basin)
...
```

**Result per covariance model:**

| Model | Î”LL with 1 seed | Î”LL with 9 seeds |
|-------|----------------|------------------|
| VVV | â‰ˆ +0.004 | â‰ˆ +0.000 |
| EII | â‰ˆ +0.0001 | â‰ˆ +0.0000 |
| VVI | â‰ˆ +0.0000 | â‰ˆ +0.0000 |

**Key insight:** Simpler models (EII, VVI) have smoother likelihood surfaces with fewer local optima, so even a single K-Means++ start finds the global optimum. The full VVV model has more parameters and more local optima, requiring multi-seed search. This is analogous to the random starts behavior we discovered in factor analysis rotations (see FA-TECHNICAL-REPORT.md, Section 8).

**Decision:** Expose `seed` as a user parameter rather than hardcoding multi-seed search. This gives users explicit control over reproducibility vs optimality, and keeps single-fit latency low (~10ms).

### 20.3 The LCA Smoothing Revelation

**Problem:** Our initial LCA implementation used Beta(1,1) smoothing for item-response probabilities:

```
Ï_km = (Î£_i z_ik Â· x_im + 1) / (N_k + 2)
```

This seemed mathematically sound â€” a uniform Beta prior prevents Ï from hitting exactly 0 or 1, which would cause log(0) = -âˆ. Cross-validation against R's poLCA revealed a Î”LL gap of 0.478 on a 2-class, 5-item dataset (N=100).

**Root cause analysis:**

```
Beta(1,1) smoothing with N_k â‰ˆ 50:
  True Ï = 0.90 â†’ estimated Ï = (45+1)/(50+2) = 0.885   (bias of -0.015)
  True Ï = 0.10 â†’ estimated Ï = (5+1)/(50+2) = 0.115    (bias of +0.015)

Over 5 items Ã— 2 classes Ã— 100 observations, these biases compound multiplicatively
in the log-likelihood: each observation's LL shifts by Î£_m [xÂ·log(0.885/0.90) + (1-x)Â·log(0.115/0.10)]
â†’ systematic downward bias of ~0.005 per observation â†’ 0.478 total.
```

**Fix:** Switch to raw MLE with minimal floor instead of prior smoothing:

```
Ï_km = clip(Î£_i z_ik Â· x_im / N_k, [1e-10, 1-1e-10])
```

**Result:** Î”LL dropped from 0.478 to **7.6 Ã— 10â»â¹** â€” exact match to 10 decimal places. The 1e-10 floor is far enough from any realistic data value to never affect the estimate, while preventing the log(0) catastrophe.

**Lesson learned:** In EM for mixture models, smoothing priors that seem benign can introduce substantial bias at moderate sample sizes. Always cross-validate against a reference implementation before assuming correctness.

### 20.4 The Entropy Convention Puzzle

**Problem:** Our initial entropy implementation used raw entropy `E = -Î£_i Î£_k z_ik Â· log(z_ik)`, which is unbounded (scales with N and K). Cross-validation against mclust revealed a mismatch â€” mclust reports entropy in [0, 1].

**Discovery:** mclust uses **normalized** entropy:

```
E_norm = 1 + Î£_i Î£_k z_ik Â· log(z_ik) / (N Â· log(K))
       = 1 - E_raw / (N Â· log(K))
```

The denominator NÂ·log(K) is the maximum possible entropy (uniform posterior across K classes for all N observations). This normalizes to [0, 1] where:
- E_norm = 1: every observation has a one-hot posterior (perfect separation)
- E_norm = 0: every observation has a uniform posterior (no clustering signal)

**Critical subtlety:** The ICL criterion uses **raw** entropy, not normalized:

```
ICL = BIC + 2 Â· E_raw          â† raw entropy
E_norm = 1 - E_raw / (NÂ·log K)  â† normalized, reported in diagnostics
```

Confusing the two would produce wildly wrong ICL values. We split the implementation into two functions: `computeRawEntropy()` (internal, for ICL) and `computeNormalizedEntropy()` (public, for diagnostics).

### 20.5 The mclust BIC Sign Convention

**Problem:** When first comparing BIC values between Carm and R mclust, the values were off by a factor of -1. Our BIC was ~760, R's was ~-760.

**Root cause:** R's mclust follows a non-standard BIC convention where **higher is better** (returns negative BIC). The standard statistical convention is **lower is better**.

**Fix in R reference generation:**

```r
bic = -fit$bic  # Convert from mclust convention to standard convention
```

**Where the DF is hidden:** mclust stores degrees of freedom not in the main fit object but as an attribute: `attr(fit$bic, "df")`. This is poorly documented and was only discovered by inspecting the mclust source code.

### 20.6 The poLCA Data Format Trap

**Problem:** poLCA silently produced wrong results when given 0/1 binary data.

**Root cause:** poLCA requires categorical variables coded as **1/2** (not 0/1). The `probs` output contains P(value=1) and P(value=2) â€” where "value=2" corresponds to our original "value=1".

**Fix in R reference script:**

```r
lca_df <- as.data.frame(lca_data_raw + 1)     # Convert 0/1 to 1/2
rho_r[k, d] <- fit$probs[[d]][k, 2]           # Column 2 = P(original=1)
```

Carm works with 0/1 directly, which is more natural for binary data and avoids this confusion.

### 20.7 Float64Array for Hot Accumulation Loops

**Problem:** Under `noUncheckedIndexedAccess: true`, the standard `number[]` type makes every index access return `number | undefined`, requiring non-null assertions (`arr[i]!`) everywhere. More importantly, `number[]` arrays have type-checking overhead that matters in tight inner loops.

**Decision:** Use `Float64Array` for:
- The responsibility matrix `resp` (N Ã— K, accessed NÃ—K times per E-step)
- Weight and count accumulators `Nk`, `weights` (K elements, accessed NÃ—K times)
- The distance matrix in silhouette/DBSCAN (NÂ² elements)

**Why Float64Array over number[]:**
1. **Typed**: No undefined checks needed, `arr[i]!` is guaranteed to be a number
2. **Dense**: Contiguous memory, better cache performance
3. **Pre-zeroed**: `new Float64Array(n)` initializes to 0.0, no `.fill(0)` needed
4. **No GC pressure**: Single allocation, no per-element boxing

**Gotcha under `noUncheckedIndexedAccess`:** The compound assignment `arr[i] += x` fails because `arr[i]` is `number | undefined`. Must use `arr[i]! += x` (or `arr[i] = arr[i]! + x`). This affects all `+=`, `-=`, `*=`, `++` operators.

### 20.8 Ward.D2: Squared Distances and the Square Root

**Problem:** HAC with Ward linkage produced heights that were systematically wrong â€” off by a factor that looked like a square root.

**Root cause:** R's `hclust(method = "ward.D2")` uses **squared Euclidean** distances internally for the Lance-Williams update, but reports merge heights as the **square root** of the merge distance. The `.D2` suffix stands for "D-squared" â€” it operates on squared distances and then takes the root.

**Fix:**

```typescript
// Internal: use squared Euclidean for Lance-Williams updates
// Output: heights[i] = Math.sqrt(mergeDistance[i])
```

**Result:** After this fix, all 29 merge heights for the 30-point test dataset matched R to 1e-10 (machine precision).

### 20.9 HAC Reference Extraction: Never Hand-Transcribe

**Problem:** The first cross-validation attempt for HAC had 10 correct heights followed by 19 wrong ones. The wrong values were plausible (monotonically increasing, reasonable magnitudes) but slightly off. Debugging took hours because the algorithm appeared to be working but producing subtly different numbers.

**Root cause:** The reference heights were copy-pasted from an R console output rather than extracted programmatically. The first 10 values happened to be from the correct dataset; the remaining 19 were from a previous session's different dataset.

**Fix:** All reference data is now extracted via `jsonlite::write_json(digits = 10)` and read directly from JSON files â€” no hand transcription, no copy-paste. This principle (`LEARNINGS.md`: "always extract programmatically â€” never type-transcribe") prevented multiple subsequent errors.

### 20.10 The depmixS4 Non-Determinism Problem

**Problem:** For LTA cross-validation, the natural R reference is depmixS4 (the standard HMM package). However, depmixS4 produces different results on every run, even with `set.seed()`.

**Root cause:** depmixS4 uses internal randomization that is not controlled by R's `set.seed()`. This makes it impossible to generate stable reference values.

**Decision:** LTA is validated through:
1. Structural tests (posteriors sum to 1, transition rows sum to 1, trajectories are valid state indices)
2. Behavioral tests (diagonal dominance in transition matrix, convergence)
3. Small hand-computed examples
4. Future: cross-validate against seqHMM package or Python's hmmlearn

### 20.11 The priorControl() Gap on Real-World Data

**Problem:** On synthetic data with well-separated clusters, Carm matches mclust exactly. On the real-world engagement dataset (717 students, overlapping clusters), there's a systematic Î”LL â‰ˆ +0.6 (Carm slightly higher).

**Root cause:** R's mclust calls `Mclust(data, prior = priorControl())` by default, which adds a conjugate prior regularization. This prior:
1. Shrinks covariance estimates toward a common pooled estimate
2. Shrinks means toward the grand mean
3. Adds a prior penalty to the log-likelihood

Carm uses covariance regularization (`regCovar = 1e-6` on the diagonal) but does not implement the full conjugate prior. On well-separated data the prior has negligible effect. On overlapping data it adds measurable regularization, explaining the small Î”LL.

**Decision:** Accept this difference rather than implementing `priorControl()`. The differences are small (Î”LL = 0.6 on 717 observations, means within 0.015, entropy within 0.003) and Carm's approach is more interpretable â€” users can control regularization explicitly via `regCovar`.

### 20.12 Empty Cluster Re-seeding Strategy

**Problem:** During K-Means, if a cluster loses all its members (typically because two nearby clusters merge), the centroid becomes undefined and the algorithm can converge with fewer than K non-empty clusters.

**Solution:** When count[j] = 0 after the assignment step, find the point that is farthest from its currently assigned centroid and reassign it to the empty cluster:

```
farIdx = argmax_i ||x_i - c_{labels[i]}||Â²
c_empty = x_{farIdx}
```

**Why farthest-from-own-centroid?** This point is the most poorly served by the current clustering â€” it's likely an outlier or a point on the boundary between clusters. Placing a new centroid there gives the empty cluster the best chance of attracting nearby points in the next assignment step.

**Alternative considered:** Random re-seeding (pick a random point). This was rejected because it doesn't exploit the structure of the problem â€” a random point is equally likely to be well-served or poorly-served by the current clustering.

### 20.13 Convergence Criterion Design

**Problem:** There are multiple reasonable convergence criteria for EM:
1. |â„“(Î¸_t) - â„“(Î¸_{t-1})| < tol (absolute log-likelihood change)
2. |â„“(Î¸_t) - â„“(Î¸_{t-1})| / |â„“(Î¸_{t-1})| < tol (relative change)
3. max|Î¸_t - Î¸_{t-1}| < tol (parameter change)

**Decision:** Use absolute log-likelihood change (option 1) with tol = 1e-6. This matches R's mclust and GPArotation convergence criteria. Relative change (option 2) is problematic when â„“ â‰ˆ 0. Parameter change (option 3) requires defining a norm across heterogeneous parameters (weights, means, covariances).

### 20.14 LTA Smoothing Calibration

**Problem:** The LTA M-step without smoothing can produce degenerate transition matrices (rows with zero entries), especially with small N or short T. But too much smoothing distorts the learned dynamics.

**Calibration:**

```
Ï€: Dirichlet(1) prior â€” (piAcc + 1) / (N + K)
   Strength: 1 pseudo-observation per state. Mild, prevents zero initial probs.

Ï„: Additive smoothing 0.1 â€” (tauNum + 0.1) / (tauDen + 0.1Â·K)
   Strength: equivalent to 0.1 pseudo-transitions per cell. Very mild.
   Without: Ï„ entries can be exactly 0, causing log(0) in forward pass.

Ï: Floor at [1e-10, 1-1e-10]
   Only prevents numerical log(0), no distributional effect.
```

These values were chosen to be small enough to not measurably affect results on reasonable datasets (N â‰¥ 30, T â‰¥ 3), while preventing the numerical catastrophes that arise on edge cases.

---

## 21. Mathematical Tricks That Made It Possible

Building a full clustering suite from scratch â€” no LAPACK, no BLAS, no numerical libraries â€” requires replacing standard library calls with mathematically equivalent formulations that are implementable in pure TypeScript. This section documents the key mathematical tricks.

### 21.1 Trick: Jacobi Eigendecomposition Instead of LAPACK

**The challenge:** GMM requires eigendecomposition of DÃ—D covariance matrices at every M-step (6 models Ã— K components = up to 6K decompositions per iteration). Standard implementations call LAPACK's `dsyev`, which is Fortran code unavailable in the browser.

**The trick:** Carm implements the **Jacobi eigenvalue algorithm** â€” iterative 2Ã—2 rotations that zero off-diagonal elements one at a time. For a symmetric matrix A:

```
1. Find the largest off-diagonal element |A[p][q]|
2. Compute rotation angle: tan(2Î¸) = 2A[p][q] / (A[p][p] - A[q][q])
3. Apply Givens rotation: A â† G(p,q,Î¸)' Â· A Â· G(p,q,Î¸)
4. Accumulate eigenvectors: V â† V Â· G(p,q,Î¸)
5. Repeat until max|off-diagonal| < 1e-12
```

**Why Jacobi works here:** Covariance matrices are symmetric positive-definite (after regularization) and typically small (D â‰¤ 30 in most applications). Jacobi converges quadratically for symmetric matrices, requiring O(DÂ²) sweeps of O(DÂ²) operations each â€” total O(Dâ´). For D=3 (engagement data), this is 81 operations per sweep, converging in 3-5 sweeps. For D=31 (factor analysis), it's ~1M operations â€” still sub-millisecond.

**What we gain over QR iteration:** Jacobi produces eigenvectors as a byproduct (the accumulated Givens rotations), while QR iteration requires a separate backward substitution phase. For our use case (need both eigenvalues AND eigenvectors), Jacobi is more natural.

### 21.2 Trick: Log-PDF Without Matrix Inversion

**The standard formula:**

```
log ğ’©(x|Î¼,Î£) = -Â½[DÂ·log(2Ï€) + log|Î£| + (x-Î¼)'Î£â»Â¹(x-Î¼)]
```

This requires Î£â»Â¹ (O(DÂ³) to compute) and |Î£| (O(DÂ³) via LU decomposition).

**The eigendecomposition trick:** Given Î£ = UÎ›U', where Î› = diag(Î»â‚,...,Î»_D):

```
Î£â»Â¹ = U Â· diag(1/Î»â‚,...,1/Î»_D) Â· U'
log|Î£| = Î£_j log(Î»_j)
(x-Î¼)'Î£â»Â¹(x-Î¼) = Î£_j (y_j)Â²/Î»_j    where y = U'(x-Î¼)
```

**Total cost:** O(DÂ²) for the projection y = U'(x-Î¼), then O(D) for the sum. No matrix inversion, no determinant. The eigendecomposition is computed once per M-step and reused N times per E-step.

**Numerical bonus:** Clamping eigenvalues to `regCovar` (1e-6) simultaneously:
- Prevents log(Î») = -âˆ (determinant underflow)
- Prevents 1/Î» = âˆ (inverse explosion)
- Guarantees Î£ is positive-definite

### 21.3 Trick: Log-Sum-Exp for Mixture Marginals

**The problem:** The marginal p(x) = Î£_k Ï€_k Â· ğ’©(x|Î¼_k,Î£_k) involves summing probabilities that can span 300 orders of magnitude. In D=10 dimensions with well-separated clusters, p(x|k=correct) â‰ˆ exp(-10) while p(x|k=wrong) â‰ˆ exp(-500).

**The trick:** Work entirely in log-space using the log-sum-exp identity:

```
log p(x) = log Î£_k exp(a_k)    where a_k = log Ï€_k + log ğ’©(x|Î¼_k,Î£_k)
         = max(a) + log Î£_k exp(a_k - max(a))
```

After subtracting max(a), all exponents are â‰¤ 0, so exp(Â·) âˆˆ (0, 1]. The sum is â‰¥ 1 (the max term contributes exp(0) = 1). No overflow, no underflow, full precision.

**The posterior trick:** The responsibility z_ik = exp(a_k - log p(x)) is computed by subtracting the log-marginal from the log-numerator. This is exact â€” no division of small numbers by small numbers.

**Where it's critical:** In LTA, the forward variable Î±_T(s) for a sequence of T=10 timepoints multiplies T emission probabilities and T-1 transition probabilities. Without log-space, this product would underflow to 0.0 for sequences as short as T=5.

### 21.4 Trick: Lance-Williams Recurrence for HAC

**The problem:** Naively recomputing inter-cluster distances after every merge requires O(nÂ²Â·D) per merge â€” O(nÂ³Â·D) total for n-1 merges. With n=1000 and D=10, this is 10 billion operations.

**The trick:** The Lance-Williams recurrence updates the distance between a newly merged cluster (iâˆªj) and any existing cluster k using only three pre-computed distances:

```
d(iâˆªj, k) = Î±_iÂ·d(i,k) + Î±_jÂ·d(j,k) + Î²Â·d(i,j) + Î³Â·|d(i,k) - d(j,k)|
```

**Cost:** O(n) per merge (update distances to all remaining clusters), O(nÂ²) total. This is a 10,000Ã— speedup over naive recomputation for n=1000.

**The Ward coefficients are dynamic:** Unlike single/complete/average linkage where coefficients are constants, Ward's method uses cluster-size-dependent coefficients:

```
Î±_i = (n_i + n_k) / (n_i + n_j + n_k)
Î±_j = (n_j + n_k) / (n_i + n_j + n_k)
Î² = -n_k / (n_i + n_j + n_k)
```

This requires tracking cluster sizes throughout the merge process â€” a detail that's easy to miss and produces subtly wrong results if forgotten.

### 21.5 Trick: K-Means++ with Running Minimum Distances

**The standard K-Means++ formula:**

```
P(selecting x_i as next center) = DÂ²(x_i) / Î£_j DÂ²(x_j)
where DÂ²(x_i) = min_{c âˆˆ selected} ||x_i - c||Â²
```

Naively computing DÂ²(x_i) requires O(NÂ·j) distance computations when selecting the j-th center, for a total of O(NÂ·KÂ²).

**The trick:** Maintain a running `dists[i]` array (Float64Array) with the minimum squared distance from each point to any selected center. After adding center j:

```
for each i: dists[i] = min(dists[i], ||x_i - c_j||Â²)
```

**Cost:** O(NÂ·K) total â€” one pass through all N points per center selection. The `min` operation is O(1) per point, and `dists` is pre-initialized to `Infinity`.

### 21.6 Trick: Covariance Constraint via Eigenvalue Manipulation

**The insight:** All six mclust covariance models can be implemented by manipulating eigenvalues after the M-step, without special-casing the E-step formula.

| Model | After eigendecomposing Î£Ì‚_k = UÎ›U' |
|-------|--------------------------------------|
| VVV | Keep Î›, keep U |
| VVI | Keep diagonal of Î£Ì‚_k, set U = I |
| VII | Set all eigenvalues to trace(Î£Ì‚_k)/D, set U = I |
| EEE | Pool: Î£Ì‚ = Î£_k Ï€_kÂ·Î£Ì‚_k, eigendecompose once, share |
| EEI | Pool diagonals, share across components |
| EII | Pool trace/D, share across components |

**What this avoids:** No separate E-step implementations, no separate log-PDF functions, no conditional branching in the inner loop. The same `mvnLogPdf(x, mu, uFlat, eigenvals, d)` function handles all six models â€” only the eigenvalues and eigenvectors change.

### 21.7 Trick: Viterbi in Log-Space

**The standard Viterbi recursion:**

```
v_t(s) = max_p [v_{t-1}(p) Â· Ï„_{ps}] Â· B_t(s)
```

This involves multiplying T probabilities, which underflows to 0 for T > 5.

**The log-space trick:**

```
log v_t(s) = log B_t(s) + max_p [log v_{t-1}(p) + log Ï„_{ps}]
```

All operations are additions in log-space. The `max` operation is numerically exact (no approximation), so the Viterbi path is identical to what you'd get in probability space.

**Why max instead of logSumExp:** The forward algorithm needs the full marginal (sum over all paths), so it uses logSumExp. Viterbi needs only the best path (max over all paths), so it uses max. Both are numerically stable in log-space.

### 21.8 Trick: Cophenetic Correlation Without Explicit Ultrametric

**The problem:** Computing the cophenetic correlation requires the cophenetic distance c(i,j) â€” the merge height at which observations i and j first join the same cluster. Naively, this requires traversing the dendrogram for every pair â€” O(nÂ²Â·n) = O(nÂ³).

**The trick:** Process merges in order. For each merge of clusters A and B at height h, every pair (a,b) where a âˆˆ A and b âˆˆ B has cophenetic distance h. Using a union-find structure to track cluster membership, the cophenetic distance matrix is built in O(nÂ²) total.

The cophenetic correlation is then the Pearson correlation between the N(N-1)/2 original distances and cophenetic distances â€” a standard dot-product computation.

### 21.9 Trick: Silhouette Without Explicit Cluster Enumeration

**The standard formula:**

```
a(i) = mean_{j âˆˆ same cluster, jâ‰ i} d(i,j)
b(i) = min_{C â‰  C_i} mean_{j âˆˆ C} d(i,j)
```

This requires knowing which points belong to each cluster â€” but we already have the `labels` array.

**Implementation trick:** For each point i, iterate through all points j exactly once. If `labels[j] === labels[i]`, accumulate into a(i). Otherwise, accumulate into cluster-specific sums (keyed by `labels[j]`). After the loop, divide each sum by its count and take the minimum for b(i).

**Noise handling:** Points with label = -1 are excluded entirely (skipped in both numerator and denominator). Their silhouette score is NaN. The mean silhouette excludes NaN values. This matches R's `cluster::silhouette` convention for DBSCAN noise points.

---

## Appendix A: Test Infrastructure

### A.1 Test Files

| File | Tests | Scope |
|------|-------|-------|
| `tests/stats/clustering.test.ts` | ~50 | Unit tests: structure, convergence, determinism, edge cases |
| `tests/stats/clustering-xval.test.ts` | ~30 | Cross-validation: R mclust, poLCA, kmeans, entropy |
| `tests/stats/clustering-engagement.test.ts` | 11 | Real-world: engagement data vs mclust VVI K=3 |
| `tests/stats/hac.test.ts` | 29 | HAC: 4 linkages, cophenetic, silhouette (all vs R hclust) |
| `tests/stats/dbscan.test.ts` | 20 | DBSCAN: structure, noise, vs R dbscan |

### A.2 R Reference Files

| File | Content |
|------|---------|
| `tests/r_clustering_reference.R` | Generates GMM/LCA/KMeans references |
| `tests/r_clustering_reference.json` | Frozen numerical results (10-digit precision) |
| `tests/engagement_mclust_ref.json` | Engagement data + mclust VVI K=3 results |
| `tests/r_hac_reference.R` | Generates HAC references (4 linkages) |
| `tests/r_hac_reference.json` | Frozen HAC heights, cophenetic, silhouette |
| `tests/r_dbscan_reference.R` | Generates DBSCAN references |
| `tests/r_dbscan_reference.json` | Frozen DBSCAN labels, point types |

### A.3 Vitest Pass Status

All tests pass as of the latest build (496/496 total, including clustering).

---

## Appendix B: Implementation Completeness

| Component | Status | Lines | Evidence |
|-----------|--------|-------|----------|
| GMM EM (6 covariance models) | âœ“ | 263â€“494 | R mclust match, 3 models tested |
| GMM prediction | âœ“ | 499â€“535 | Posterior + hard labels for new data |
| GMM model selection | âœ“ | 546â€“564 | BIC grid search over K Ã— model |
| GMM range fitting | âœ“ | 1085â€“1101 | For BIC curve visualization |
| LCA (Bernoulli EM) | âœ“ | 585â€“691 | poLCA match to 10 decimals |
| LTA (Baum-Welch + Viterbi) | âœ“ | 729â€“950 | Structure tests, no R ref (depmixS4 non-deterministic) |
| K-Means (Lloyd + K-Means++) | âœ“ | 967â€“1054 | R kmeans exact match |
| K-Means prediction | âœ“ | 1128â€“1146 | Nearest centroid |
| K-Means range fitting | âœ“ | 1111â€“1126 | For elbow plot |
| DBSCAN | âœ“ | 1307â€“1400 | R dbscan match |
| HAC (4 linkages) | âœ“ | 1489â€“1834 | R hclust match to 1e-10 |
| Silhouette scores | âœ“ | 1199â€“1262 | R cluster::silhouette match |
| Distance matrix | âœ“ | 1161â€“1181 | Float64Array, O(nÂ²) |
| Cophenetic correlation | âœ“ | 1770â€“1835 | R match |
| Entropy (mclust convention) | âœ“ | 50â€“69 | Normalized, raw, case-specific |
| AvePP | âœ“ | 71â€“83 | Per-component posterior certainty |
| PRNG (splitmix32) | âœ“ | 23â€“35 | Deterministic, seed=42 |
| logSumExp | âœ“ | 39â€“48 | Prevents underflow in all EM models |

**Total:** 1,850 lines of implementation + ~850 lines of tests + ~170 lines of R reference scripts.

---

## Appendix C: Known Limitations

1. **GMM initialization:** K-Means++ may find different local optima than mclust's hierarchical initialization. Multi-seed search (9+ seeds) is needed for reliable convergence on complex datasets.

2. **LTA cross-validation:** R's depmixS4 is non-deterministic, preventing automated cross-validation. Manual verification on small synthetic examples confirms correctness. Future work: cross-validate against seqHMM or hmmlearn.

3. **mclust priorControl():** R's mclust uses a conjugate prior regularization by default. Carm uses covariance regularization (regCovar = 1e-6) instead. This causes small differences on real-world data (Î”LL â‰ˆ 0.6, Î”BIC â‰ˆ 1.3 on 717-observation engagement data).

4. **Computational complexity:** The distance matrix for silhouette and DBSCAN is O(nÂ²) in memory and time. For datasets with N > 10,000, approximate methods would be needed.

5. **Missing covariance models:** mclust supports 14 covariance parameterizations; Carm implements 6 (VVV, EEE, VVI, EEI, VII, EII). The remaining 8 (EVV, VEV, VVE, etc.) involve more complex decompositions but are rarely needed in practice.
